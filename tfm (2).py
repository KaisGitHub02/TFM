# -*- coding: utf-8 -*-
"""TFM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LgdL1yvyQCUf1NaC4XHSwNB7SL-wzoN_
"""

#@title Librerias
!pip install -U langchain langchain-community langchain-huggingface transformers huggingface_hub sentence-transformers scikit-learn streamlit

"""# Modelos habilitados

## Funcionamiento del script de verificaci√≥n de modelos de LLM

Este script realiza una verificaci√≥n de modelos de lenguaje disponibles en Hugging Face para identificar cu√°les tienen endpoints accesibles para generaci√≥n de texto. A continuaci√≥n se explica su estructura y funcionamiento:

### 1. **Configuraci√≥n inicial**
- Importa bibliotecas necesarias para interactuar con Hugging Face Hub
- Configura el token de API para autenticaci√≥n
- Crea un cliente de inferencia y una lista vac√≠a para almacenar modelos accesibles

### 2. **Obtenci√≥n de modelos**
- Obtiene los 100 modelos m√°s populares (`sort="downloads"`) de generaci√≥n de texto
- Filtra solo los que soportan text-generation

### 3. **Verificaci√≥n de accesibilidad**
- Para cada modelo:
  - Obtiene metadatos detallados con `model_info`
  - Realiza una prueba de generaci√≥n con prompt simple
  - Si la prueba es exitosa, a√±ade el modelo a la lista accesible

### 4. **Manejo de errores y controles**
- Captura errores espec√≠ficos de conexi√≥n HTTP
- Maneja excepciones generales
- Incluye pausa de 1 segundo entre solicitudes para evitar sobrecarga

### 5. **Resultados finales**
- Muestra lista final de modelos con endpoints funcionales
- Proporciona output claro para su uso posterior

**Caracter√≠sticas clave:**
- Verificaci√≥n en dos pasos (metadata + prueba real)
- Sistema de logging con iconos visuales (‚úÖ/‚ùå)
- Optimizado para rendimiento (limit=100, delays controlados)
- Filtrado por modelos populares y especializados

Este script sirve como base para identificar modelos viables para experimentos comparativos en sistemas de debate entre LLMs, permitiendo filtrar solo aquellos con endpoints operativos.

"""

from huggingface_hub import list_models, model_info, InferenceClient
from huggingface_hub.utils import HfHubHTTPError
import time

# Reemplaza esto con tu clave de API de Hugging Face
api_token = "Xxxxxxxx"

# Inicializa el cliente de inferencia con tu clave de API
client = InferenceClient(provider="hf-inference", token=api_token)

# Lista para almacenar modelos accesibles
accessible_models = []

# Obtiene modelos que admiten la tarea de generaci√≥n de texto
models = list_models(filter="text-generation", sort="downloads", limit=200)

for model in models:
    model_id = model.modelId
    try:
        # Intenta obtener informaci√≥n detallada del modelo
        info = model_info(model_id, token=api_token)

        # Verifica si el modelo tiene un endpoint de inferencia accesible
        # Intentamos generar un texto simple para verificar la accesibilidad
        test_prompt = "Hello"
        try:
            response = client.text_generation(prompt=test_prompt, model=model_id, max_new_tokens=5)
            print(f"‚úÖ Modelo accesible: {model_id}")
            accessible_models.append(model_id)
        except Exception as e:
            print(f"‚ùå Error al acceder al modelo {model_id}: {e}")

        # Espera breve para evitar sobrecargar la API
        time.sleep(1)

    except HfHubHTTPError as http_err:
        print(f"‚ùå Error HTTP al obtener informaci√≥n del modelo {model_id}: {http_err}")
    except Exception as e:
        print(f"‚ùå Error al procesar el modelo {model_id}: {e}")

# Muestra la lista de modelos accesibles
print("\nModelos con endpoints accesibles para generaci√≥n de texto:")

for model_id in accessible_models:
    print(f"- {model_id}")

"""# Librerias y funcion auxiliar

Este c√≥digo implementa un sistema robusto de generaci√≥n de texto con gesti√≥n autom√°tica de errores y reintentos inteligentes para interactuar con modelos de lenguaje de Hugging Face. A continuaci√≥n se desglosa su estructura clave:

### 1. **Configuraci√≥n inicial y par√°metros**
- Par√°metros esenciales:
  - `max_retries`: N√∫mero m√°ximo de reintentos (3 por defecto)
  - `wait_time`: Tiempo base de espera entre intentos (5 segundos)
  - `fallback_model`: Modelo alternativo para failover
  - **kwargs: Par√°metros personalizables de generaci√≥n

### 2. **Mecanismo de reintentos**
- Estrategia de retroceso exponencial adaptativo:
  - Espera inicial de `wait_time`
  - Duplica el tiempo de espera en errores de sobrecarga
  - L√≥gica de fallback autom√°tico a modelo alternativo

### 3. **Gesti√≥n avanzada de errores**
- Detecta y clasifica errores:
  - HTTP (ej. 503 Service Unavailable)
  - Modelo no disponible
  - Errores de validaci√≥n de entrada
- Registro detallado con `traceback` para diagn√≥stico

### 4. **Funcionalidades especiales**
- Parada autom√°tica con `stop_sequences`
- Truncamiento inteligente de respuestas
- Compatibilidad con m√∫ltiples secuencias de parada

**Caracter√≠sticas clave del dise√±o:**
- Tolerancia a fallos mediante patr√≥n Retry con backoff exponencial
- Failover autom√°tico a modelos alternativos
- Registro detallado de errores con contexto t√©cnico
- Adaptaci√≥n din√°mica a diferentes tipos de errores
- Configuraci√≥n flexible de par√°metros de generaci√≥n

Este sistema permite una interacci√≥n robusta con endpoints de modelos LLM, ideal para entornos productivos donde la estabilidad y recuperaci√≥n de errores son cr√≠ticas. La implementaci√≥n sigue mejores pr√°cticas para sistemas distribuidos, particularmente relevante en arquitecturas de microservicios.

"""

# Importaciones est√°ndar y de Hugging Face
from huggingface_hub import InferenceClient, login
import time
import sys
import json # Para decodificar errores JSON
import itertools # Para combinaciones de modelos
import re # Para dividir en frases
import requests

# Importaciones para an√°lisis sem√°ntico
from sentence_transformers import SentenceTransformer, util
import numpy as np

print("Librer√≠as importadas.")
import traceback

def generate_with_retry(client, prompt, max_retries=3, wait_time=5, fallback_model=None, **kwargs):
    default_params = {"max_new_tokens": 1000, "temperature": 0.7, "do_sample": True}
    generation_params = {**default_params, **kwargs}
    if 'stop_sequences' in generation_params and not isinstance(generation_params['stop_sequences'], list):
        generation_params['stop_sequences'] = [generation_params['stop_sequences']]

    model_identifier = getattr(client, 'model', 'Desconocido')
    attempt = 1
    fallback_used = False

    while attempt <= max_retries:
        try:
            print(f"  Enviando a {model_identifier} (Intento {attempt}/{max_retries}, Endpoint: text_generation)...")
            response = client.text_generation(prompt, **generation_params)

            print(f"  Respuesta recibida de {model_identifier} (Endpoint: text_generation).")
            if 'stop_sequences' in generation_params:
                for stop_seq in generation_params['stop_sequences']:
                    if stop_seq in response:
                        response = response.split(stop_seq)[0]
                        print(f"  Texto truncado por stop_sequence: '{stop_seq}'")

            return response.strip()

        except Exception as e:
            # Captura traceback completo y tipo de error
            full_trace = traceback.format_exc()
            error_type = type(e).__name__

            # Si es un error HTTP conocido
            if hasattr(e, 'response') and hasattr(e.response, 'status_code'):
                status = e.response.status_code
                url = e.response.url if hasattr(e.response, 'url') else "URL desconocida"
                try:
                    detail = e.response.text
                except:
                    detail = "<No se pudo leer el cuerpo de la respuesta>"

                error_message = (
                    f"‚ùå [{error_type}] Error HTTP {status} al contactar {url}\n"
                    f"---\n{detail.strip()[:500]}\n---"
                )
            else:
                error_message = f"‚ùå [{error_type}] {str(e)}\n---\n{full_trace.strip().splitlines()[-5:]}\n---"

            print(f"  üîç Error detallado en intento {attempt}/{max_retries} para {model_identifier}:\n{error_message}")

            if "not supported for task" in str(e).lower():
                print("  ‚ö†Ô∏è Modelo no soporta text-generation. Intentando con modelo alternativo...")
                if fallback_model and not fallback_used:
                    client.model = fallback_model['model']
                    model_identifier = client.model
                    attempt = 1
                    fallback_used = True
                    continue

            if any(k in str(e).lower() for k in ["503", "currently loading", "unavailable", "overloaded"]):
                print("  ‚è≥ Modelo sobrecargado o no disponible. Esperando m√°s tiempo...")
                time.sleep(wait_time * 2)
            elif "Input validation error" in str(e):
                print("  ‚ùó Error de validaci√≥n. Verifica prompt/par√°metros.")
                return f"Error de validaci√≥n en {model_identifier}:\n{error_message}"

            if attempt == max_retries:
                return f"Error definitivo en {model_identifier} tras {max_retries} intentos:\n{error_message}"

            print(f"  Reintentando en {wait_time} segundos...")
            time.sleep(wait_time)
            attempt += 1

    return f"Error: Se agotaron los reintentos para {model_identifier}."

"""# Autenticaci√≥n con Hugging Face

Este c√≥digo implementa un sistema seguro de autenticaci√≥n para Hugging Face Hub con verificaci√≥n de token y manejo de errores.

- Incluye mecanismo de advertencia para tokens no configurados:
- Detecta tokens placeholder y muestra instrucciones claras
- Implementa autenticaci√≥n program√°tica con huggingface_hub.login()
"""

HF_TOKEN = "Xxxx"

if HF_TOKEN.startswith("hf_..."):
    print("‚ö†Ô∏è ADVERTENCIA: Por favor, reemplaza 'hf_...' con tu token real de Hugging Face.")
    print("     Puedes obtener uno en: https://huggingface.co/settings/tokens")
    # Alternativamente, en Colab, ve a la pesta√±a 'Secretos' (icono de llave)
    # y a√±ade un secreto llamado HF_TOKEN con tu token como valor.
    # Luego, descomenta la siguiente l√≠nea y comenta la anterior:
    # from google.colab import userdata
    # HF_TOKEN = userdata.get('HF_TOKEN')
    # sys.exit("Token no configurado. Deteniendo ejecuci√≥n.") # Opcional: detener si no hay token

try:
    login(token=HF_TOKEN)
    print("‚úÖ Sesi√≥n iniciada correctamente con Hugging Face.")
except Exception as e:
    print(f"‚ùå Error al iniciar sesi√≥n en Hugging Face: {str(e)}")
    print("   Aseg√∫rate de que el token sea v√°lido y tenga los permisos necesarios.")
    # Podr√≠as querer detener la ejecuci√≥n aqu√≠ si el login falla
    # sys.exit("Fallo en el login. Deteniendo ejecuci√≥n.")

"""# Configuraci√≥n del Sistema de Debate entre LLMs

Este c√≥digo establece la arquitectura base para un sistema de debate entre modelos de lenguaje.

### 1. **Configuraci√≥n de Modelos y Roles**
- **Estructura de modelos**:
  - Clave: Nombre personalizado del modelo
  - Valor: Dict con:
    - `model`: Identificador HF Hub
    - `original_role`: Posici√≥n inicial en el debate (Pro/Contra/Neutral)

- **Tema personalizable**: Variable `topic` permite modificar el debate

### 2. **Funci√≥n de Obtenci√≥n de Modelos Accesibles**
- **Caracter√≠sticas clave**:
  - Cacheo de resultados por 1 hora (`ttl=3600`)
  - Filtrado combinado (metadata + test pr√°ctico)
  - L√≠mite ajustable de modelos a verificar
"""

# Funci√≥n para seleccionar opci√≥n
def select_option(prompt, options, allow_multiple=False):
    print(prompt)
    for idx, item in enumerate(options, 1):
        print(f"{idx}) {item}")
    while True:
        user_input = input("Selecciona una opci√≥n (o varias separadas por comas): " if allow_multiple else "Selecciona una opci√≥n: ")
        try:
            if allow_multiple:
                selected = [int(x.strip()) for x in user_input.split(",")]
                if all(1 <= x <= len(options) for x in selected):
                    return [options[x-1] for x in selected]
                else:
                    print("Algunas opciones no son v√°lidas. Intenta de nuevo.")
            else:
                selected = int(user_input)
                if 1 <= selected <= len(options):
                    return options[selected-1]
                else:
                    print("Opci√≥n no v√°lida. Intenta de nuevo.")
        except ValueError:
            print("Por favor, ingresa un n√∫mero v√°lido.")

# Pedir al usuario cu√°ntos modelos quiere usar
while True:
    num_models = input("¬øCu√°ntos modelos quieres usar en el debate? (m√≠nimo 1): ")
    try:
        num_models = int(num_models)
        if num_models < 1 or num_models > len(accessible_models):
            print(f"Por favor, ingresa un n√∫mero entre 1 y {len(accessible_models)}.")
        else:
            break
    except ValueError:
        print("Por favor, ingresa un n√∫mero v√°lido.")

# Seleccionar los modelos y roles
models_config = {}
for i in range(1, num_models + 1):
    print(f"\nSelecci√≥n del modelo {i}:")
    selected_model = select_option("Elige un modelo:", accessible_models)
    print(f"Modelo seleccionado: {selected_model}")
    role = input(f"Rol para el modelo {selected_model} (ej: Pro, Contra, Neutral): ")
    models_config[f"Modelo_{i}"] = {
        "model": selected_model,
        "original_role": role
    }

# Pedir el tema del debate
topic = input("\nIngresa el tema del debate: ")

# Mostrar configuraci√≥n final
print("\nConfiguraci√≥n de modelos:")
for name, config in models_config.items():
    print(f"  - {name}: {config['model']} (Rol: {config.get('original_role', 'No asignado')})")
print(f"\nTema del debate: {topic}")

"""# Inicializaci√≥n de Clientes de Inferencia para Debate entre LLMs

Este c√≥digo implementa un sistema robusto para inicializar clientes de inferencia de modelos de lenguaje, con verificaci√≥n en tiempo real y manejo de errores estructurado. A continuaci√≥n se detalla su funcionamiento:

### 1. **Configuraci√≥n B√°sica**
- **Estructuras de datos**:
  - `clients`: Diccionario para almacenar clientes inicializados
  - `available_models`: Registro de modelos disponibles tras verificaci√≥n

### 2. **Bucle de Inicializaci√≥n**
- **Proceso por modelo**:
  - Extrae ID del modelo desde la configuraci√≥n
  - Intenta crear cliente con autenticaci√≥n (`token=HF_TOKEN`)
  - Almacena cliente y metadata si es exitoso
  - Captura y registra errores espec√≠ficos

### 3. **Mecanismos Clave**
| Caracter√≠stica | Implementaci√≥n | Beneficio |
|----------------|----------------|-----------|
| Autenticaci√≥n | `token=HF_TOKEN` | Seguridad y control de acceso [6] |
| Verificaci√≥n impl√≠cita | Creaci√≥n de cliente + prueba opcional | Detecta modelos no disponibles [2] |
| Manejo de errores | `try-except` anidado | Previene fallos en cascada |
| Registro detallado | Mensajes con iconos/estados | Diagn√≥stico r√°pido de problemas |

### 4. **Verificaci√≥n Post-Inicializaci√≥n**
- **Chequeo cr√≠tico**:
  - Verifica existencia de al menos un cliente funcional
  - Provee feedback visual inmediato
  - Opci√≥n para detener ejecuci√≥n si falla (comentada)

**Flujo de trabajo:**
1. Iterar sobre configuraci√≥n de modelos
2. Intentar conexi√≥n con cada endpoint
3. Registrar estado (√©xito/fallo)
4. Reportar resumen final

Este componente es esencial para garantizar que solo modelos operativos participen en el debate, mejorando la estabilidad del sistema. La implementaci√≥n sigue mejores pr√°cticas para sistemas distribuidos, particularmente relevante en entornos de producci√≥n con m√∫ltiples proveedores de modelos .

"""

# --- Crear Clientes ---
print("\n‚è≥ Inicializando clientes de inferencia...")
clients = {}
available_models = {} # Guardar√° solo los modelos que se inicialicen correctamente

for model_name, info in models_config.items():
    model_id = info["model"]
    print(f"  Intentando inicializar {model_name} ({model_id})...", end="")
    try:
        clients[model_name] = InferenceClient(provider="hf-inference", model=model_id, token=HF_TOKEN)
        available_models[model_name] = info
        print(f" ‚úÖ OK")
    except Exception as e:
        print(f" ‚ùå ERROR: {str(e)}")
        print(f"      El modelo '{model_name}' no estar√° disponible para el debate.")

if not clients:
    print("\n‚ùå FATAL: No se pudo inicializar ning√∫n cliente de inferencia. Revisa los errores y tu token.")
else:
    print("\n‚úÖ Clientes inicializados para los modelos disponibles:")
    for name in clients.keys():
        print(f"  - {name}")
print("-" * 50)

"""# Sistema de Generaci√≥n de Respuestas para Debate entre LLMs

Este c√≥digo implementa el n√∫cleo del sistema de debate, gestionando la generaci√≥n de argumentos diferenciados por rol y almacenando los resultados. Aqu√≠ se detalla su estructura:

### 1. **Configuraci√≥n Inicial del Debate**
- **Elementos clave**:
  - Diccionario `responses` para almacenar respuestas en memoria
  - Archivo `debate_responses.txt` para registro persistente
  - Encabezado estructurado con tema del debate

### 2. **Plantillas de Prompt por Rol**
| Rol | Caracter√≠sticas del Prompt | Ejemplo de Instrucci√≥n |
|-----|----------------------------|------------------------|
| **Pro** | Enfoque en beneficios y justificaci√≥n | _"Fundamenta por qu√© la propuesta es correcta..."_ |
| **Contra** | An√°lisis de riesgos y desventajas | _"Conc√©ntrate en los riesgos y falacias..."_ |
| **Neutral** | Listado objetivo de argumentos | _"Enumera los argumentos clave a favor y en contra..."_ |


### 3. **Par√°metros Clave de Generaci√≥n**
| Par√°metro | Valor | Efecto |
|-----------|-------|--------|
| `temperature` | 0.7 | Balance creatividad/coherencia |
| `max_new_tokens` | 1024 | L√≠mite extensi√≥n respuestas |
| `wait_time` | 5s | Espera entre reintentos |

**Caracter√≠sticas destacadas:**
1. Generaci√≥n diferenciada por roles (3 plantillas especializadas)
2. Sistema anti-hallucinaciones con detecci√≥n de anomal√≠as
3. Registro dual (memoria + archivo f√≠sico)
4. Visualizaci√≥n progresiva de resultados
5. Adaptabilidad a diferentes modelos mediante configuraci√≥n

Este m√≥dulo constituye el coraz√≥n del sistema de debate, permitiendo comparar objetivamente las capacidades argumentativas de diferentes LLMs bajo condiciones controladas.


"""

# --- Generaci√≥n de Respuestas con Prompts Diferenciados ---
print("\n=== INICIANDO DEBATE ABIERTO ===")
print(f"TEMA: {topic}\n")

responses = {} # Diccionario para almacenar las respuestas

# --- Abrir archivo para guardar respuestas ---
output_file = open("debate_responses.txt", "w", encoding="utf-8")
output_file.write(f"=== RESPUESTAS DEL DEBATE ABIERTO ===\n")
output_file.write(f"Tema: {topic}\n")

intro_guardrail = (
    "IMPORTANTE: Esta es una tarea formal de argumentaci√≥n estructurada. "
    "No incluyas saludos, introducciones, conclusiones, comentarios meta, notas editoriales, enlaces ni frases de cierre. "
    "Responde √∫nicamente con argumentos expresados en oraciones completas. "
    "No repitas argumentos ni incluyas ning√∫n comentario adicional o explicaci√≥n fuera de los argumentos.\n\n"
)

debate_prompt_template_pro = (
    intro_guardrail +
    "**Tema del Debate:** {topic}\n"
    "**Rol:** Participante experto A FAVOR de la premisa indicada.\n"
    "**Instrucciones:** Exp√≥n tus argumentos clave a favor en p√°rrafos desarrollados, sin listas, sin encabezados, sin saludos ni conclusiones. No a√±adas ning√∫n texto fuera de los argumentos.\n"
    "**Ejemplo de formato:**\n"
    "El avance tecnol√≥gico en la educaci√≥n permite personalizar el aprendizaje y mejorar el acceso a recursos de calidad. Adem√°s, fomenta la motivaci√≥n de los estudiantes al adaptar los contenidos a sus intereses y necesidades, incrementando as√≠ el rendimiento acad√©mico.\n\n"
    "Comienza tu respuesta directamente con el primer argumento:\n"
)

contra_prompt_template = (
    intro_guardrail +
    "**Tema del Debate:** {topic}\n"
    "**Rol:** Participante experto EN CONTRA de la premisa indicada.\n"
    "**Instrucciones:** Exp√≥n tus argumentos clave en contra en p√°rrafos desarrollados, sin listas, sin encabezados, sin saludos ni conclusiones. No a√±adas ning√∫n texto fuera de los argumentos.\n"
    "**Ejemplo de formato:**\n"
    "La adopci√≥n de tecnolog√≠a en la educaci√≥n puede aumentar la brecha digital, ya que no todos los estudiantes tienen acceso a dispositivos o conexi√≥n estable. Adem√°s, la dependencia excesiva de herramientas tecnol√≥gicas puede afectar negativamente la socializaci√≥n y el desarrollo de habilidades interpersonales.\n\n"
    "Comienza tu respuesta directamente con el primer argumento:\n"
)

neutral_prompt_template = (
    intro_guardrail +
    "**Tema del Debate:** {topic}\n"
    "**Rol:** Observador neutral y objetivo.\n"
    "**Instrucciones:** Enumera los argumentos clave a favor y en contra como una lista simple con guiones (-), usando oraciones completas. No incluyas encabezados, saludos, conclusiones ni ning√∫n texto adicional.\n"
    "**Ejemplo de formato:**\n"
    "- El acceso a la educaci√≥n digital permite llegar a zonas rurales.\n"
    "- La falta de infraestructura tecnol√≥gica dificulta la igualdad de oportunidades.\n\n"
    "Comienza tu respuesta directamente con la lista de argumentos:\n"
)



debate_prompt_template_pro = intro_guardrail + debate_prompt_template_pro
contra_prompt_template = intro_guardrail + contra_prompt_template
neutral_prompt_template = intro_guardrail + neutral_prompt_template
# --- Generaci√≥n ---
for model_name, info in available_models.items():
    role = info.get('original_role', 'Pro')
    print(f"‚è≥ Generando respuesta de {model_name} (Rol asignado: {role})...")
    client = clients[model_name]
    role_desc_for_print = f"{model_name} ({role})"

    # --- SELECCI√ìN DEL PROMPT BASADA EN EL ROL ---
    if role == "Contra":
        prompt = contra_prompt_template.format(topic=topic)
        print(f"    Usando prompt espec√≠fico para rol 'Contra'.")
    elif role == "Neutral":
        prompt = neutral_prompt_template.format(topic=topic)
        print(f"    Usando prompt espec√≠fico para rol 'Neutral'.")
    else: # Por defecto o si el rol es 'Pro'
        prompt = debate_prompt_template_pro.format(topic=topic)
        print(f"    Usando prompt para rol 'Pro' / por defecto.")

    # --- Llamada a la API con el prompt seleccionado ---
    response = generate_with_retry(
        client,
        prompt,
        temperature=0.9,
        max_new_tokens=1024,
        wait_time=5
    )

    # --- !! Limpieza Espec√≠fica para Gemma (o rol Neutral) MEJORADA !! ---
    original_response_len = len(response)
    is_truncated = False
    # --- Limpieza general para cortar si hay otra instrucci√≥n o cambio de tema ---
    if not response.startswith("Error:"):
        anomaly_markers = [
            "Instrucci√≥n", "Instrucci√≥n 2", "Tema del Debate:",
            "Tu Rol:", "Tarea:", "Pregunta de Seguimiento",
            "Soluci√≥n:", "Resumen de Argumentos", "Desarrollo de tu Perspectiva"
        ]
        import re
        min_pos = -1
        for marker in anomaly_markers:
            match = re.search(re.escape(marker), response[50:], re.IGNORECASE)
            if match:
                pos = match.start() + 50
                if min_pos == -1 or pos < min_pos:
                    min_pos = pos

        if min_pos != -1:
            print(f"    ‚ö†Ô∏è Detectada posible anomal√≠a en la respuesta en la posici√≥n {min_pos}. Truncando...")
            response = response[:min_pos].strip()
            is_truncated = True
            print(f"    Longitud original: {original_response_len}, Longitud truncada: {len(response)}")

    responses[model_name] = response # Guardar respuesta original o truncada

    # --- Guardar respuesta en el archivo ---
    status = "OK"
    if is_truncated:
        status = "OK (Truncada)"
    elif response.startswith("Error:"):
        status = "ERROR"

    output_file.write(f"--- Respuesta de {model_name} ---\n")
    output_file.write(f"Rol: {role}\n")
    output_file.write(f"Estado: {status}\n")
    output_file.write(f"Longitud: {len(response)} caracteres\n")
    output_file.write(f"Respuesta:\n{response}\n")
    output_file.write("-" * 50 + "\n\n")

    # --- Imprimir resultado parcial ---
    if not response.startswith("Error:"):
        print(f"‚úÖ Respuesta generada por {model_name}. ({status})")
        response_preview = response.replace('\n', ' ').strip()
        print(f"\nüîπ {role_desc_for_print}:\n{response_preview[:400]}...\n")
    else:
        print(f"‚ùå Error al generar respuesta de {model_name}.")
        print(f"\nüîπ {role_desc_for_print}: {response}\n")
    print('-'*50)

# --- Cerrar archivo ---
output_file.close()
print("\n‚úÖ Respuestas guardadas en 'debate_responses.txt'")

print("\n=== GENERACI√ìN INICIAL COMPLETADA ===")
print("Respuestas obtenidas:")
for name, resp in responses.items():
    role_assigned = available_models.get(name, {}).get('original_role', 'N/A')
    status = "OK" if not resp.startswith("Error:") else "ERROR"
    if name in responses and responses[name] != resp:  # Esto no deber√≠a ocurrir, pero por si acaso
        status += " (Modificada)"
    print(f"  - {name} (Rol: {role_assigned}): {status} ({len(resp)} caracteres)")

"""# Extracci√≥n de Argumentos Clave

Este c√≥digo realiza la extracci√≥n y desduplicaci√≥n sem√°ntica de argumentos clave a partir de las respuestas generadas por modelos de lenguaje en un debate. Permite obtener una lista consolidada y √∫nica de argumentos, facilitando su an√°lisis posterior.

### 1. Funciones Principales

#### 1.1. Desduplicaci√≥n Sem√°ntica
- Objetivo: Eliminar argumentos que sean sem√°nticamente similares.

- T√©cnica: Utiliza embeddings y similitud coseno para detectar duplicados.

- Salida: Devuelve argumentos √∫nicos y una lista de duplicados descartados.

#### 1.2. Extracci√≥n de Argumentos con LLMs

- Objetivo: Extraer los argumentos clave de cada respuesta usando un LLM.

- Prompt: Instrucciones claras para que el modelo enumere los argumentos.

- Filtrado: Elimina l√≠neas irrelevantes usando listas de palabras clave.

### 2. Procesamiento de las Respuestas

- Procesa cada respuesta v√°lida.
- Llama a la funci√≥n de extracci√≥n de argumentos.
- Guarda el origen de cada argumento (modelo que lo gener√≥).

### 3. Desduplicaci√≥n y Guardado

- Elimina argumentos sem√°nticamente similares, para ello:
 - Convertir cada argumento en un vector num√©rico (embedding) usando un modelo de transformaci√≥n de texto a vector.
 - Cada argumento se representa como un vector de 384 dimensiones.
 - Comparar cada par de vectores mediante producto escalar (similitud coseno).
 - Normalizar por el producto de sus magnitudes.
 - Iterar por cada argumento principal.
 - Comparar con todos los posteriores.
 - Si similitud > umbral ‚Üí marcar como duplicado


- Escribe los argumentos √∫nicos y los duplicados descartados en un archivo TXT.
"""

import uuid
from sentence_transformers import SentenceTransformer, util
import numpy as np
import re
import time

print("\n=== EXTRACCI√ìN DE ARGUMENTOS CLAVE ===")

def deduplicate_arguments(arguments, threshold=0.7):
    if len(arguments) <= 1:
        return arguments, []
    embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')
    embeddings = embedder.encode(arguments, convert_to_tensor=True)
    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings).cpu().numpy()
    keep = []
    used = set()
    duplicates = []
    for i in range(len(arguments)):
        if i not in used:
            keep.append(arguments[i])
            for j in range(i + 1, len(arguments)):
                if similarity_matrix[i][j] > threshold:
                    print(f"   ‚ö†Ô∏è Argumento descartado como duplicado: '{arguments[j][:100]}...' (similitud: {similarity_matrix[i][j]:.3f}, conservado: '{arguments[i][:100]}...')")
                    used.add(j)
                    duplicates.append((arguments[j], arguments[i], similarity_matrix[i][j]))
    return keep, duplicates

def extract_key_arguments(client, text, topic, model_name_for_log="Extractor", max_retries=3, wait_time=5):
    extractor_client = next((client for name, client in clients.items() if name in models_config), None)
    model_name_for_log = models_config[list(clients.keys())[0]]['model'] if clients else "Desconocido"
    if not extractor_client:
        print("   ‚ùå No hay cliente LLM disponible para extracci√≥n.")
        return []

    prompt = f"""
**Tarea:** Eres un analista experto. Lee el siguiente texto que es una opini√≥n sobre el tema "{topic}".
Extrae los **argumentos o puntos clave principales** presentados en el texto.
Enumera cada argumento de forma concisa y clara en una l√≠nea separada, empezando con un guion '- '.
No a√±adas introducciones, conclusiones ni comentarios. Solo la lista de argumentos.

**Texto a analizar:**
{text[:3000]}

**Argumentos Clave Extra√≠dos:**
- """

    print(f"   ‚è≥ Extrayendo argumentos del texto (usando {model_name_for_log})...")
    raw_extraction = generate_with_retry(
        extractor_client,
        prompt,
        max_new_tokens=500,
        temperature=0.3,
        stop=["## Fin de Extracci√≥n ##"],
        wait_time=wait_time,
        max_retries=3
    )

    if raw_extraction.startswith("Error:") or not raw_extraction.strip():
        print(f"   ‚ö†Ô∏è Respuesta cruda para depuraci√≥n: {raw_extraction[:2000]}...")
        print(f"   ‚ùå Error extrayendo argumentos: {raw_extraction}")
        return []

    extracted_points = []
    lines = raw_extraction.strip().split('\n')
    irrelevant_keywords = [
        'instrucci√≥n', 'tarea', 'argumentos clave', 'texto a analizar', 'enumera', 'no a√±adas',
        'debate sobre', 'siguiente texto', 'pregunta de seguimiento', 'clasifica', 'proporciona una',
        'evaluaci√≥n detallada', 'evaluaci√≥n cr√≠tica', 'pol√≠tica p√∫blica', 'para cada argumento',
        'categor√≠as espec√≠ficas', 'relaciona con la categor√≠a', 'identifica y categoriza', 'evidencia',
        'impacto potencial', 'posibles consecuencias', 'comparaci√≥n', 'perspectiva √©tica', 'soluci√≥n',
        'importante:', 'desarrollo de tu perspectiva', 'las redes sociales han transformado',
        'las plataformas digitales han revolucionado', 'en primer lugar', 'en segundo lugar', 'en tercer lugar',
        'restricciones adicionales', 'incluye', 'excluye', 'a√±ade', 'organiza', 'documento a analizar',
        'datos estad√≠sticos', 'ejemplos espec√≠ficos', 'sesgos o limitaciones'
    ]
    relevant_keywords = ['privacidad', 'desinformaci√≥n', 'seguridad', 'libertad', 'equidad', 'innovaci√≥n', 'protecci√≥n', 'menores', 'ciberacoso', 'noticias falsas', '√©tica', 'censura', 'econom√≠a']
    for line in lines:
        clean_line = line.strip()
        clean_line = re.sub(r'^\d+\.\s*', '', clean_line)
        clean_line = re.sub(r'\*\*.*?\*\*:\s*', '', clean_line)
        clean_line = re.sub(r'^\*\*Soluci√≥n \d+:\*\*\s*', '', clean_line)
        clean_line = re.sub(r'\*\*[A-Z]{2}\*\*', '', clean_line)
        if clean_line.startswith('- '):
            point = clean_line[2:].strip()
        else:
            point = clean_line.strip()
        if (len(point) > 20 and
            not any(keyword in point.lower() for keyword in irrelevant_keywords) or
            any(keyword in point.lower() for keyword in relevant_keywords)):
            extracted_points.append(point)
        else:
            print(f"   ‚ö†Ô∏è L√≠nea descartada: {clean_line[:150]}...")

    if not extracted_points:
        print(f"   ‚ö†Ô∏è Respuesta cruda para depuraci√≥n: {raw_extraction[:2000]}...")
    else:
        print(f"   ‚úÖ Argumentos extra√≠dos: {len(extracted_points)}")
        for point in extracted_points:
            print(f"   - {point[:150]}...")

    return extracted_points

# --- Extracci√≥n para cada respuesta v√°lida ---
all_arguments_raw = []
argument_source = {}

valid_responses_for_extraction = {name: resp for name, resp in responses.items() if resp and not resp.startswith("Error:")}

for model_name, response_text in valid_responses_for_extraction.items():
    print(f"\nProcesando respuesta de: {model_name}")
    print(f"   Respuesta original (primeros 500 caracteres): {response_text[:500]}...")
    args = extract_key_arguments(None, response_text, topic, model_name_for_log=models_config[model_name]['model'])
    if args:
        all_arguments_raw.extend(args)
        for arg in args:
            if arg not in argument_source:
                argument_source[arg] = model_name
    time.sleep(2)

# --- Consolidar y Desduplicar Argumentos ---
unique_arguments, duplicates_info = deduplicate_arguments(all_arguments_raw, threshold=0.9)
arguments_for_evaluation = {str(uuid.uuid4()): arg_text for arg_text in unique_arguments}

print("\n--- Argumentos √önicos Consolidados para Evaluaci√≥n ---")
if arguments_for_evaluation:
    for arg_id, arg_text in arguments_for_evaluation.items():
        source_model = argument_source.get(arg_text, "M√∫ltiples?")
        print(f"   ID: {arg_id[:8]}... | Origen: ~{source_model} | Texto: {arg_text[:100]}...")
else:
    print("   No se pudieron extraer argumentos clave.")

print(f"\nTotal de argumentos √∫nicos para evaluar: {len(arguments_for_evaluation)}")

# --- Guardar en archivo TXT ---
with open("argumentos_extraidos.txt", "w", encoding="utf-8") as f:
    f.write("ARGUMENTOS √öNICOS:\n\n")
    for arg in unique_arguments:
        source = argument_source.get(arg, "M√∫ltiples?")
        f.write(f"- {arg} (Origen: {source})\n")

    f.write("\n\nARGUMENTOS DESCARTADOS POR DUPLICADOS:\n\n")
    for dup, original, sim in duplicates_info:
        f.write(f"- {dup} (duplicado de: \"{original}\" | similitud: {sim:.3f})\n")

print("‚úÖ Resultados guardados en 'argumentos_extraidos.txt'")

"""# Simulaci√≥n de Preferencias Individuales

### 1. **Funci√≥n Principal: `rate_arguments_by_llm`**
- Eval√∫a argumentos usando un LLM como "votante virtual"
- Genera puntuaciones num√©ricas (1-10) para cada argumento
- **Novedad:** Maneja respuestas mal formateadas completando con la media

### 2. **Flujo de Trabajo Mejorado**

#### 2.1. Extracci√≥n de N√∫meros
- **Regex:** `r'\d+'` captura todos los n√∫meros en la respuesta
- Ejemplo:  
  `Respuesta: "Los mejores son 8, 9 y maybe 7" ‚Üí [8, 9, 7]`

#### 2.2. Normalizaci√≥n de Rango
- Fuerza valores al rango 1-10
- Ejemplo:  
  `[0, 15] ‚Üí [1, 10]`

#### 2.3. Completado con Media Aritm√©tica
- **C√°lculo de media:** Promedio de los valores extra√≠dos
- **Relleno inteligente:** Completa los faltantes con la media
- Ejemplo:  
  `Argumentos necesarios: 5 | Valores extra√≠dos: [7,9] ‚Üí [7,9,8,8,8]`

### 3. **Manejo de Respuestas Mal Formateadas**

| Caso | Acci√≥n | Ejemplo |
|------|--------|---------|
| **Sin n√∫meros** | Rellena con 5 | `[] ‚Üí [5,5,5]` |
| **Menos n√∫meros** | Completa con media | `[8,6] ‚Üí [8,6,7,7]` |
| **M√°s n√∫meros** | Recorta lista | `[7,8,9,5] ‚Üí [7,8,9]` (si necesitamos 3) |
| **Valores extremos** | Normaliza a 1-10 | `[0,15] ‚Üí [1,10]` |

### 4. **Ventajas Clave**
1. **Robustez:** Funciona incluso con respuestas desestructuradas
2. **Consistencia:** Garantiza siempre el n√∫mero correcto de puntuaciones
3. **Equilibrio:** El relleno con media mantiene coherencia estad√≠stica
4. **Flexibilidad:** Adaptable a diferentes modelos de lenguaje



"""

import re
import time

print("\n=== EVALUACI√ìN ITERATIVA DE ARGUMENTOS ===")

# Calibraci√≥n: ejemplos de referencia para ayudar al modelo a entender la escala
calibration_samples = [
    ("Argumento contradictorio y fuera de tema.", 2),
    ("Argumento v√°lido pero poco desarrollado.", 5),
    ("Argumento s√≥lido, claro y bien fundamentado.", 9)
]

def build_prompt(argumento, voter_name, topic, calibration_samples):
    prompt = (
        f"ROL: Eres '{voter_name}', un evaluador objetivo en un debate sobre '{topic}'.\n"
        f"TAREA: Califica el siguiente argumento con un n√∫mero del 1 al 10 seg√∫n estos criterios:\n"
        "1. Claridad (1=incomprensible, 10=perfectamente claro)\n"
        "2. Coherencia l√≥gica (1=contradictorio, 10=perfectamente estructurado)\n"
        "3. Relevancia (1=irrelevante, 10=totalmente centrado)\n"
        "Instrucciones:\n"
        "- S√© cr√≠tico, no seas generoso.\n"
        "- Usa valores bajos (1-3) para argumentos deficientes, medios (4-7) para promedio y altos (8-10) solo para excepcionales.\n"
        "- Considera el argumento objetivamente, sin sesgo.\n"
        "- Responde SOLO con un n√∫mero del 1 al 10.\n\n"
        "Ejemplos de calibraci√≥n:\n"
    )
    for ejemplo, score in calibration_samples:
        prompt += f"- \"{ejemplo}\" => {score}\n"
    prompt += f"\nArgumento a evaluar:\n\"{argumento}\"\n\nRESPUESTA: "
    return prompt

def normalize_score(score, model_name):
    # Puedes ajustar los sesgos conocidos de cada modelo aqu√≠ si lo deseas
    model_bias = {
        "Phi-3.5-mini-instruct": -1,
        "Llama-3.1-8B": +1,
        # A√±ade ajustes para otros modelos si es necesario
    }
    return max(1, min(10, score + model_bias.get(model_name, 0)))

def extract_score(respuesta):
    # Extrae el primer n√∫mero entre 1 y 10
    match = re.search(r'\b(10|\d)\b', respuesta)
    if match:
        return int(match.group())
    return None

def rate_arguments_one_by_one(
    client,
    voter_name,
    arguments_dict,
    topic,
    max_retries=3,
    wait_time=8
):
    if not arguments_dict:
        print("   ‚ö†Ô∏è No hay argumentos para evaluar.")
        return None

    ratings = {}
    arg_ids = list(arguments_dict.keys())

    for idx, arg_id in enumerate(arg_ids, 1):
        argumento = arguments_dict[arg_id]
        prompt = build_prompt(argumento, voter_name, topic, calibration_samples)

        for attempt in range(1, max_retries + 1):
            print(f"   üìù Argumento {idx}/{len(arg_ids)} | Intento {attempt}/{max_retries}")
            respuesta = generate_with_retry(
                client,
                prompt,
                max_new_tokens=15,
                temperature=0.8,
                stop=["\n"],
                wait_time=wait_time
            )

            print(f"DEBUG: Respuesta cruda -> '{respuesta}'")

            puntuacion = extract_score(respuesta)
            if puntuacion is not None:
                ratings[arg_id] = puntuacion
                print(f"   ‚úÖ Puntuaci√≥n aceptada: {puntuacion}")
                break
            else:
                print(f"   ‚ùå Formato inv√°lido en intento {attempt}")
                if attempt == max_retries:
                    ratings[arg_id] = 5  # Valor por defecto
                    print("   ‚ö†Ô∏è Usando valor neutro (5) tras fallos")
        time.sleep(1)  # Espacio entre evaluaciones

    return ratings

# --- Implementaci√≥n en el flujo principal ---
individual_preferences = {}

if 'arguments_for_evaluation' in locals() and arguments_for_evaluation:
    if 'clients' in locals() and 'valid_responses_for_extraction' in locals() and 'topic' in locals():
        for model_name, client_instance in clients.items():
            if model_name in valid_responses_for_extraction:
                print(f"\nüîç Evaluando con: {model_name}")
                ratings = rate_arguments_one_by_one(
                    client_instance,
                    model_name,
                    arguments_for_evaluation,
                    topic
                )
                if ratings:
                    individual_preferences[model_name] = ratings
    else:
        print("Error: Faltan variables requeridas")
else:
    print("No hay argumentos para evaluar")

# Visualizaci√≥n de resultados
print("\n--- RESULTADOS FINALES ---")
for model, prefs in individual_preferences.items():
    print(f"\n{model}:")
    for arg_id, score in list(prefs.items())[:3]:  # Muestra primeros 3
        print(f"  - {arg_id[:8]}: {score}/10")

"""# Consenso OWA

### 1. **Funciones Clave**

#### 1.1. `owa_aggregate(values, weights)`
- **Prop√≥sito:** Aplica el operador OWA (Ordered Weighted Averaging)
- **Mec√°nica:**
  1. Ordena los valores de mayor a menor
  2. Aplica los pesos seg√∫n la posici√≥n ordenada
- **Ejemplo:**  
  `values = [8,5,9], weights = [0.5,0.3,0.2] ‚Üí 0.5*9 + 0.3*8 + 0.2*5 = 8.1`

#### 1.2. `generate_owa_weights(n, alpha)`
- **Par√°metro Alpha:** Controla la distribuci√≥n de pesos:
  - `alpha > 1`: Pesos decrecientes (prioriza valores altos)
  - `alpha = 1`: Pesos uniformes (equivalente a media aritm√©tica)
  - `alpha < 1`: Pesos crecientes (prioriza valores bajos)

### 2. **C√°lculo de Preferencia Colectiva**
- **Estrategia:** Agregaci√≥n OWA con pesos generados din√°micamente
- **Valor por defecto:** 5 para argumentos no evaluados
- **Ejemplo de pesos (n=3, alpha=1):**  
  `[0.333, 0.333, 0.333] ‚Üí Media aritm√©tica`

### 3. **Medici√≥n de Consenso**
- **Base te√≥rica:** Usa varianza respecto al valor OWA como indicador de disenso [3]
- **Normalizaci√≥n:**  
  `Consenso = 1 - (Varianza observada / Varianza m√°xima te√≥rica)`
- **Interpretaci√≥n:**  
  - 1: Consenso total (varianza 0)
  - 0: M√°ximo desacuerdo posible

### 4. **Par√°metros Clave**

| Par√°metro | Valor por defecto | Efecto |
|-----------|-------------------|--------|
| `alpha` | 1.0 | Controla optimismo/pesimismo de la agregaci√≥n |
| `max_possible_var` | 20.25 | Varianza m√°xima te√≥rica para rango 1-10 |
| `owa_weights` | Generadas din√°micamente | Determina influencia de cada votante seg√∫n posici√≥n ordenada |


"""

import numpy as np

print("\n=== C√ÅLCULO DE PREFERENCIA COLECTIVA Y CONSENSO CON OWA ===")

def owa_aggregate(values, weights):
    values_sorted = sorted(values, reverse=True)
    return sum(w * v for w, v in zip(weights, values_sorted))

def generate_owa_weights(n, alpha=1.0):
    ranks = np.arange(1, n + 1)
    weights = (n - ranks + 1) ** alpha
    weights /= weights.sum()
    return weights

collective_preference = {}
consensus_level = 0.0
consensus_details = "No calculado."
consensus_vars = {}  # <--- Para guardar la varianza de cada argumento

if individual_preferences and len(individual_preferences) >= 2:
    num_voters = len(individual_preferences)
    argument_ids = list(arguments_for_evaluation.keys())

    alpha = 1.0
    owa_weights = generate_owa_weights(num_voters, alpha=alpha)

    print(f"   Calculando preferencia colectiva (OWA, alpha={alpha:.2f})...")
    for arg_id in argument_ids:
        ratings = [prefs.get(arg_id, 5) for prefs in individual_preferences.values()]
        agg_value = owa_aggregate(ratings, owa_weights)
        collective_preference[arg_id] = agg_value

    print("   Calculando nivel de consenso (varianza respecto a valor OWA)...")
    consensus_diffs = []
    for arg_id in argument_ids:
        ratings = [prefs.get(arg_id, 5) for prefs in individual_preferences.values()]
        owa_val = collective_preference[arg_id]
        var = np.mean([(r - owa_val) ** 2 for r in ratings])
        consensus_vars[arg_id] = var  # <--- Guardar varianza individual
        consensus_diffs.append(var)

    avg_owa_var = np.mean(consensus_diffs)
    max_possible_var = ((10 - 1) ** 2) / 4  # ~20.25
    normalized_owa_var = min(avg_owa_var / max_possible_var, 1.0)

    consensus_level = round(1.0 - normalized_owa_var, 3)
    consensus_details = (
        f"Consenso basado en varianza respecto a OWA (avg var: {avg_owa_var:.2f}, normalizado: {normalized_owa_var:.2f}) ‚Üí Nivel: {consensus_level:.3f}"
    )

    print(f"   Preferencia Colectiva (Top 5):")
    sorted_collective = sorted(collective_preference.items(), key=lambda x: x[1], reverse=True)
    for i, (arg_id, avg_rating) in enumerate(sorted_collective[:5]):
        print(f"     {i+1}. {avg_rating:.2f} | ID: {arg_id[:8]} | {arguments_for_evaluation[arg_id][:70]}...")

    # --- NUEVO: Mostrar los argumentos con menor consenso (mayor varianza) ---
    print(f"\n   Argumentos con menor consenso (mayor varianza):")
    sorted_by_var = sorted(consensus_vars.items(), key=lambda x: x[1], reverse=True)
    for i, (arg_id, var) in enumerate(sorted_by_var[:5]):
        print(f"     {i+1}. Var: {var:.2f} | ID: {arg_id[:8]} | {arguments_for_evaluation[arg_id][:70]}...")

    print(f"\n   {consensus_details}")

else:
    print("   No hay suficientes votantes para calcular consenso.")

"""# Sistema de Ranking Final y Negociaci√≥n Aut√≥noma entre LLMs

## 1. **Arquitectura General**
Este c√≥digo implementa un sistema de consenso autom√°tico para debates entre m√∫ltiples LLMs, combinando t√©cnicas de:
- Agregaci√≥n de preferencias colectivas
- Medici√≥n cuantitativa de consenso
- Negociaci√≥n iterativa basada en desacuerdos


## 2. **Funciones Clave**

### 2.1. `simulate_negotiation_round()`
- **Prop√≥sito:** Modificar preferencias individuales mediante debate estructurado
- **Mecanismo:**
  1. Identifica top 3 argumentos con mayor desacuerdo (std dev)
  2. Genera prompts personalizados con distribuci√≥n de puntuaciones
  3. Parsea respuestas usando regex (`NUEVA_PUNTUACION: \d+`)
  4. Actualiza ratings solo si est√°n en rango v√°lido (1-10)

### 2.2. `compute_consensus_level()`
- **F√≥rmula:**  
  `Consenso = 1 - (avg(|rating_i - avg_colectivo|) / 9)`  
  - 9 = m√°ximo rango posible (10-1)
  - Resultado: 0 (m√°ximo desacuerdo) a 1 (consenso total)

## 3. **M√©tricas de Desacuerdo**
| M√©trica | C√°lculo | Uso |
|---------|---------|-----|
| **StdDev** | `np.std(ratings)` | Identificar argumentos pol√©micos |
| **Consenso** | `1 - avg_normalized_diff` | Decidir necesidad de negociaci√≥n |
| **Threshold** | 0.65 | Umbral configurable para consenso |

## 4. **Proceso de Negociaci√≥n**
1. **Identificaci√≥n de puntos cr√≠ticos:**
   - Ordenar argumentos por desviaci√≥n est√°ndar

2. **Ronda de negociaci√≥n:**
- Cada modelo recibe feedback de otros participantes


3. **Actualizaci√≥n din√°mica:**
- Solo acepta cambios v√°lidos (1-10)
- Registra explicaciones para trazabilidad

## 5. **Generaci√≥n de Ranking Final**
- **Criterios:**
  - Promedio de ratings post-negociaci√≥n
  - Consenso validado por threshold
"""

def simulate_negotiation_round(disagreement_points, individual_preferences, arguments, clients):
    updated_preferences = {model: prefs.copy() for model, prefs in individual_preferences.items()}

    for model_name, client in clients.items():
        print(f"\nüîÅ Negociaci√≥n para modelo: {model_name}")
        for point in disagreement_points[:3]:  # Top 3 en desacuerdo
            arg_id = point["id"]
            argument_text = arguments[arg_id]
            ratings_summary = ", ".join([f"{m}: {r}" for m, r in zip(individual_preferences.keys(), point["ratings"])])
            prompt = (
                f"Otros participantes han evaluado el siguiente argumento con puntuaciones variadas:\n"
                f"Argumento: \"{argument_text}\"\n"
                f"Distribuci√≥n de puntuaciones: {ratings_summary}\n\n"
                f"Tu puntuaci√≥n anterior fue: {individual_preferences[model_name].get(arg_id, 'N/A')}\n"
                f"Por favor, reconsidera tu puntuaci√≥n. Si cambias de opini√≥n, da una nueva puntuaci√≥n del 1 al 10, "
                f"y explica brevemente por qu√©. Si mantienes tu puntuaci√≥n, tambi√©n explica por qu√©.\n"
                f"Formato de respuesta esperado: NUEVA_PUNTUACION: [1-10]. Explicaci√≥n: ..."
            )
            response = generate_with_retry(client, prompt, max_new_tokens=200)
            match = re.search(r"NUEVA_PUNTUACION\s*[:Ôºö]\s*(\d{1,2})", response)
            if match:
                new_rating = int(match.group(1))
                if 1 <= new_rating <= 10:
                    updated_preferences[model_name][arg_id] = new_rating
                    print(f"  ‚úî Nueva puntuaci√≥n para {arg_id[:8]} por {model_name}: {new_rating}")
                else:
                    print(f"  ‚ùå Puntuaci√≥n fuera de rango: {match.group(1)}")
            else:
                print(f"  ‚ö†Ô∏è No se encontr√≥ puntuaci√≥n v√°lida. Respuesta: {response}")

    return updated_preferences


print("\n=== RANKING DE ALTERNATIVAS O NEGOCIACI√ìN ===")

final_ranking = []
consensus_threshold = 0.65
needs_negotiation = False

def compute_collective_preference(preferences_dict):
    all_ids = list(next(iter(preferences_dict.values())).keys())
    collective = {}
    for arg_id in all_ids:
        ratings = [prefs[arg_id] for prefs in preferences_dict.values()]
        collective[arg_id] = np.mean(ratings)
    return collective

def compute_consensus_level(preferences_dict, collective_pref):
    diffs = []
    for prefs in preferences_dict.values():
        diffs += [abs(prefs[arg_id] - collective_pref[arg_id]) for arg_id in prefs]
    normalized_diffs = [1 - (d / 9) for d in diffs]  # M√°ximo rango = 9
    return np.mean(normalized_diffs)


# 1. Calcular preferencia colectiva y consenso
collective_preference = compute_collective_preference(individual_preferences)
consensus_level = compute_consensus_level(individual_preferences, collective_preference)

# 2. Evaluar si se necesita negociaci√≥n
if collective_preference and consensus_level >= consensus_threshold:
    print(f"‚úÖ Consenso ALCANZADO (Nivel: {consensus_level:.3f} >= {consensus_threshold}).")
    print("   Generando ranking final basado en la preferencia colectiva...")

    sorted_collective = sorted(collective_preference.items(), key=lambda item: item[1], reverse=True)

    for i, (arg_id, avg_rating) in enumerate(sorted_collective):
        final_ranking.append({
            "rank": i + 1,
            "arg_id": arg_id,
            "text": arguments_for_evaluation[arg_id],
            "avg_rating": avg_rating,
            "individual_ratings": {model: individual_preferences[model].get(arg_id, 'N/A') for model in individual_preferences}
        })
        if i < 10:
            print(f"   Rank {i+1}: {avg_rating:.2f} - {arguments_for_evaluation[arg_id][:100]}...")
            if i < 3:
                ratings_str = ", ".join([f"{m}: {r}" for m, r in final_ranking[-1]["individual_ratings"].items()])
                print(f"      Ratings: [{ratings_str}]")

elif collective_preference and consensus_level < consensus_threshold:
    print(f"‚ùå Consenso BAJO (Nivel: {consensus_level:.3f} < {consensus_threshold}).")
    needs_negotiation = True

    disagreement_points = []
    for arg_id in arguments_for_evaluation:
        ratings = [prefs.get(arg_id, 5) for prefs in individual_preferences.values()]
        std_dev = np.std(ratings)
        disagreement_points.append({"id": arg_id, "std_dev": std_dev, "ratings": ratings})

    disagreement_points.sort(key=lambda x: x['std_dev'], reverse=True)

    print("   Puntos de mayor desacuerdo:")
    for i, point in enumerate(disagreement_points[:3]):
        ratings_str = ", ".join(map(str, point['ratings']))
        print(f"     {i+1}. Arg ID: {point['id'][:8]}... (StdDev: {point['std_dev']:.2f}) | Ratings: [{ratings_str}] | Texto: {arguments_for_evaluation[point['id']][:80]}...")

    print("\n   --> Ejecutando ronda de negociaci√≥n...")
    individual_preferences = simulate_negotiation_round(disagreement_points, individual_preferences, arguments_for_evaluation, clients)

    # üîÅ Recalcular preferencias colectivas y nivel de consenso tras la negociaci√≥n
    collective_preference = compute_collective_preference(individual_preferences)
    consensus_level = compute_consensus_level(individual_preferences, collective_preference)
    print(f"\nüîÅ Nuevo nivel de consenso tras negociaci√≥n: {consensus_level:.3f}")

    if consensus_level >= consensus_threshold:
        print("‚úÖ Consenso MEJORADO. Generando ranking final...")
        sorted_collective = sorted(collective_preference.items(), key=lambda item: item[1], reverse=True)
        for i, (arg_id, avg_rating) in enumerate(sorted_collective):
            final_ranking.append({
                "rank": i + 1,
                "arg_id": arg_id,
                "text": arguments_for_evaluation[arg_id],
                "avg_rating": avg_rating,
                "individual_ratings": {model: individual_preferences[model].get(arg_id, 'N/A') for model in individual_preferences}
            })
            if i < 10:
                print(f"   Rank {i+1}: {avg_rating:.2f} - {arguments_for_evaluation[arg_id][:100]}...")
                if i < 3:
                    ratings_str = ", ".join([f"{m}: {r}" for m, r in final_ranking[-1]["individual_ratings"].items()])
                    print(f"      Ratings: [{ratings_str}]")
    else:
        print("‚ùå El consenso sigue siendo bajo despu√©s de la negociaci√≥n.")
        final_ranking = "Negociaci√≥n fallida. No se logr√≥ consenso suficiente."
else:
    print("   No se pudo determinar consenso ni ranking. Datos insuficientes.")
    final_ranking = "No disponible."

"""# Carga y Uso del Modelo de Embeddings para An√°lisis Sem√°ntico

Esta celda prepara el entorno para realizar an√°lisis sem√°ntico de frases y argumentos, cargando un modelo de embeddings multiling√ºe basado en la arquitectura Sentence Transformers.


## 1. **Modelo Utilizado**

- **Nombre:** `paraphrase-multilingual-mpnet-base-v2`
- **Tipo:** Modelo de Sentence Transformers
- **Dimensi√≥n del embedding:** 768 dimensiones
- **Capacidades:**  
  - Convierte frases y p√°rrafos en vectores densos de 768 dimensiones.
  - Permite comparar similitud sem√°ntica entre textos, realizar clustering, b√∫squeda sem√°ntica y an√°lisis cruzado entre diferentes idiomas.
  - Entrenado sobre m√°s de mil millones de pares de frases en m√∫ltiples idiomas, lo que lo hace ideal para tareas multiling√ºes.


## 2. **Funcionamiento**


- **Carga:** Se inicializa el modelo con `SentenceTransformer(embedding_model_name)`.
- **Uso:**  
  - Se puede codificar cualquier lista de frases con `embedder.encode(lista_de_frases)`.
  - El resultado es un array de vectores (uno por frase), que pueden compararse usando m√©tricas como la similitud coseno.


## 3. **Ventajas del modelo**

- **Multiling√ºe:** Soporta decenas de idiomas y mapea frases con significado similar a vectores cercanos, aunque est√©n en diferentes idiomas.
- **R√°pido y eficiente:** Adecuado para tareas de clustering, b√∫squeda sem√°ntica, deduplicaci√≥n y comparaci√≥n de argumentos.
- **Vers√°til:** Puede usarse tanto en ingl√©s como en espa√±ol (y muchos otros idiomas) sin necesidad de modificar el c√≥digo.


## 4. **Manejo de errores**

- Si hay un error al cargar el modelo, se muestra un mensaje y se desactiva el an√°lisis sem√°ntico.

## 5. **Referencias t√©cnicas**

- [Documentaci√≥n oficial y ejemplos de uso (Hugging Face)](https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2)
- [Gu√≠a sobre uso multiling√ºe de Sentence Transformers](https://milvus.io/ai-quick-reference/how-do-you-use-sentence-transformers-in-a-multilingual-setting-for-example-loading-a-multilingual-model-to-encode-sentences-in-different-languages)


"""

print("\n=== PREPARANDO AN√ÅLISIS SEM√ÅNTICO ===")

try:
    # 1. Instalar el paquete si no existe
    from importlib.util import find_spec
    if not find_spec('sentence_transformers'):
        print("‚è≥ Instalando sentence-transformers...")
        !pip install -q sentence-transformers

    # 2. Cargar modelo con manejo expl√≠cito de OOM
    from sentence_transformers import SentenceTransformer
    print("‚è≥ Cargando modelo de embeddings...")

    # Usar modelo m√°s ligero si falla el original
    try:
        embedder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
    except Exception as e:
        print(f"‚ö†Ô∏è Fall√≥ modelo principal: {e}\nIntentando con modelo alternativo...")
        embedder = SentenceTransformer('paraphrase-albert-small-v2')  # 43MB vs 1.2GB

    # Verificaci√≥n final
    if not hasattr(embedder, 'encode'):
        raise RuntimeError("El modelo no se inicializ√≥ correctamente")

    print("‚úÖ Modelo listo!")

except Exception as e:
    print(f"‚ùå Error cr√≠tico: {e}")
    embedder = None

"""# Explicaci√≥n del An√°lisis Sem√°ntico en Debates entre LLMs

Este c√≥digo implementa un sistema de an√°lisis sem√°ntico para comparar respuestas de diferentes modelos de lenguaje en un debate. A continuaci√≥n se detalla su funcionamiento:

## 1. **Flujo Principal**
1. Filtrar respuestas v√°lidas
2. Dividir en frases y calcular emeddings
3. Comparar par de modelos
4. Generar report

**Objetivo:** Identificar puntos de consenso/divergencia sem√°ntica entre las respuestas de los modelos.


## 2. **Procesamiento de Texto**
### 2.1. Divisi√≥n en Frases
- **T√©cnica:** Usa expresiones regulares para dividir por signos de puntuaci√≥n
- **Filtrado:** Elimina frases menores a 10 caracteres (ruido)
- **Fallback:** Usa el texto completo si la divisi√≥n falla

### 2.2. Generaci√≥n de Embeddings
- **Modelo:** `paraphrase-multilingual-mpnet-base-v2` (768 dimensiones)
- **Salida:** Tensor PyTorch con embeddings por frase


## 3. **Comparaci√≥n Sem√°ntica**
### 3.1. C√°lculo de Similitud Coseno
- **F√≥rmula:**
  $$\text{similitud} = \frac{A \cdot B}{\|A\| \|B\|}$$
- **Rango:** -1 (opuestos) a 1 (id√©nticos) [3][5]

### 3.2. Detecci√≥n de Puntos Clave
- **Par√°metros Clave:**
  | Par√°metro | Valor Default | Efecto |
  |-----------|---------------|--------|
  | `similarity_threshold` | 0.75 | Controla sensibilidad de detecci√≥n |
  | `max_points_per_pair` | 3 | Limita resultados por par de modelos |


## 4. **Manejo de Errores y Edge Cases**
- **Casos cubiertos:**
  - Fallos en generaci√≥n de embeddings
  - Respuestas vac√≠as o errores de formato
  - Modelos sin suficientes frases para comparar


"""

# --- AN√ÅLISIS SEM√ÅNTICO ---
print("\n=== EJECUTANDO AN√ÅLISIS SEM√ÅNTICO ===")

analysis_summary = "El an√°lisis sem√°ntico no se ejecut√≥."
common_points = []
similarity_threshold = 0.75  # Puedes ajustar este umbral

# Simulaci√≥n de respuestas para ejemplo
# responses = {"modelo1": "El f√∫tbol es importante. El deporte une a las personas.", "modelo2": "El f√∫tbol une a la sociedad. El deporte es fundamental para la salud."}

if embedder and 'responses' in locals() and responses:
    # Filtrar respuestas v√°lidas
    valid_responses = {name: resp for name, resp in responses.items() if resp and not resp.startswith("Error:")}

    if len(valid_responses) < 2:
        analysis_summary = "Se necesita al menos dos respuestas v√°lidas para el an√°lisis de similitud."
        print(analysis_summary)
    else:
        print(f"Analizando similitudes entre {len(valid_responses)} respuestas v√°lidas (Umbral > {similarity_threshold})...")
        embeddings = {}
        sentences = {}

        # 1. Dividir respuestas en frases y calcular embeddings
        print("  Calculando embeddings para las frases...")
        for name, text in valid_responses.items():
            sents = re.split(r'(?<=[.!?])\s+', text.strip())
            sents = [s.strip() for s in sents if len(s.strip()) > 10]
            if not sents:
                sents = [text]
            sentences[name] = sents
            try:
                embeddings[name] = embedder.encode(sents, convert_to_tensor=True, show_progress_bar=False)
                print(f"    Embeddings calculados para {name} ({len(sents)} frases).")
            except Exception as emb_error:
                print(f"    ‚ùå Error al calcular embeddings para {name}: {emb_error}")
                if name in embeddings: del embeddings[name]
                if name in sentences: del sentences[name]

        # 2. Comparar embeddings entre pares de modelos
        print("  Comparando frases entre modelos...")
        model_pairs = list(itertools.combinations(embeddings.keys(), 2))

        if not model_pairs:
            analysis_summary = "No hay suficientes embeddings v√°lidos para comparar."
            print(f"  {analysis_summary}")

        for model1, model2 in model_pairs:
            if model1 not in embeddings or model2 not in embeddings or len(embeddings[model1]) == 0 or len(embeddings[model2]) == 0:
                print(f"    Saltando comparaci√≥n {model1}-{model2} (faltan embeddings).")
                continue

            print(f"    Comparando {model1} vs {model2}...")
            try:
                cosine_scores = util.pytorch_cos_sim(embeddings[model1], embeddings[model2])
                hits = []
                for i in range(len(embeddings[model1])):
                    for j in range(len(embeddings[model2])):
                        score = cosine_scores[i, j].item()
                        if score >= similarity_threshold:
                            hits.append({'score': score, 'model1_idx': i, 'model2_idx': j})

                hits = sorted(hits, key=lambda x: x['score'], reverse=True)
                added_indices_m1 = set()
                added_indices_m2 = set()
                count = 0
                max_points_per_pair = 3

                for hit in hits:
                    if count >= max_points_per_pair: break
                    idx1, idx2 = hit['model1_idx'], hit['model2_idx']
                    if idx1 not in added_indices_m1 and idx2 not in added_indices_m2:
                        common_points.append(
                            f"- **{model1} y {model2}**: "
                            f"'{sentences[model1][idx1][:90]}...' <=> '{sentences[model2][idx2][:90]}...' "
                            f"(Similitud: {hit['score']:.3f})"
                        )
                        added_indices_m1.add(idx1)
                        added_indices_m2.add(idx2)
                        count += 1

            except Exception as sim_error:
                print(f"    ‚ùå Error calculando similitud entre {model1} y {model2}: {sim_error}")

        # 3. Crear el resumen del an√°lisis
        analysis_report = f"Resumen del An√°lisis de Similitud (Umbral > {similarity_threshold:.2f}):\n"
        if common_points:
            analysis_report += "Se encontraron los siguientes puntos con alta similitud sem√°ntica entre respuestas:\n"
            for point in common_points:
                analysis_report += f"{point}\n"
        else:
            if len(valid_responses) >= 2 and any(len(e)>0 for e in embeddings.values()):
                analysis_report += f"No se encontraron frases con similitud significativa por encima del umbral {similarity_threshold:.2f}.\n"
            elif len(valid_responses) < 2:
                analysis_report += "No hubo suficientes respuestas v√°lidas para comparar.\n"
            else:
                analysis_report += "No se pudieron calcular suficientes embeddings para comparar.\n"

        analysis_summary = analysis_report
        print("\n‚úÖ An√°lisis sem√°ntico completado.")
        print("\n--- Resumen del An√°lisis ---")
        print(analysis_summary)
        print("--------------------------")

elif not embedder:
    analysis_summary = "An√°lisis sem√°ntico omitido: el modelo de embeddings no se carg√≥ correctamente."
    print(analysis_summary)
elif 'responses' not in locals() or not responses:
    analysis_summary = "An√°lisis sem√°ntico omitido: no se generaron respuestas iniciales."
    print(analysis_summary)

"""# Inicializaci√≥n del Modelo Moderador

### Prop√≥sito Principal
Configurar un modelo LLM especializado para actuar como moderador en el debate, encargado de:
- Resumir argumentos clave
- Sintetizar puntos de consenso/desacuerdo
- Coordinar rondas de negociaci√≥n


#### 1. **Selecci√≥n del Modelo**
| Par√°metro              | Valor                            | Descripci√≥n                                                                 |
|------------------------|----------------------------------|-----------------------------------------------------------------------------|
| `moderator_model_id`    | mistralai/Mixtral-8x7B-Instruct  | Modelo de 8B par√°metros con arquitectura Mixtral, especializado en instrucciones |
| Alternativa            | mistralai/Mistral-7B-Instruct    | Versi√≥n m√°s ligera (7B) para entornos con recursos limitados                |

#### 2. **Inicializaci√≥n del Cliente**

- **Mecanismo:** Conexi√≥n al endpoint de Hugging Face usando el token de API
- **Gesti√≥n de Errores:**
  - Captura excepciones durante la inicializaci√≥n
  - Provee feedback detallado en caso de fallo

### Flujo de Trabajo
1. **Configuraci√≥n inicial** del modelo moderador
2. **Conexi√≥n segura** al servicio de inferencia
3. **Prueba opcional** (comentada) para verificar conectividad
4. **Notificaci√≥n de estado** (√©xito/error)



"""

# --- Configuraci√≥n e Inicializaci√≥n del Moderador ---
print("\n=== PREPARANDO AL MODERADOR ===")

# Elige qu√© modelo actuar√° como moderador (puede ser uno de los anteriores o uno diferente)
# Llama 3.1 8B suele ser bueno para tareas de resumen y s√≠ntesis
moderator_model_id = "microsoft/Phi-3-mini-4k-instruct"
# moderator_model_id = "mistralai/Mistral-7B-Instruct-v0.3" # Alternativa

moderator_client = None
print(f"‚è≥ Inicializando moderador ({moderator_model_id})...")
try:
    moderator_client = InferenceClient(provider="hf-inference", model=moderator_model_id, token=HF_TOKEN)
    # Peque√±a prueba opcional
    # moderator_client.text_generation("test", max_new_tokens=1)
    print(f"‚úÖ Cliente para el moderador ({moderator_model_id}) inicializado.")
except Exception as e:
    print(f"‚ùå ERROR al inicializar el moderador ({moderator_model_id}): {str(e)}")
    print("   No se podr√° generar el resumen del moderador.")

"""# Generaci√≥n del Resumen del Moderador

### Objetivo Principal
Generar un resumen estructurado del debate utilizando un LLM especializado como moderador, identificando:
- Argumentos clave
- Puntos de acuerdo/desacuerdo
- Conclusiones relevantes


### Componentes Clave del C√≥digo

#### 1. **Recopilaci√≥n de Respuestas**
- Filtra respuestas con errores
- Limita el contexto a 6000 caracteres para evitar sobrecarga

#### 2. **Construcci√≥n del Prompt (Versi√≥n Simplificada)**
- Obtiene los 100 modelos m√°s populares (`sort="downloads"`) de generaci√≥n de texto
- Filtra solo los que soportan text-generation

### 3. **Verificaci√≥n de accesibilidad**
- Para cada modelo:
  - Obtiene metadatos detallados con `model_info`
  - Realiza una prueba de generaci√≥n con prompt simple
  - Si la prueba es exitosa, a√±ade el modelo a la lista accesible

### 4. **Manejo de errores y controles**
- Captura errores espec√≠ficos de conexi√≥n HTTP
- Maneja excepciones generales
- Incluye pausa de 1 segundo entre solicitudes para evitar sobrecarga

### 5. **Resultados finales**
- Muestra lista final de modelos con endpoints funcionales
- Proporciona output claro para su uso posterior

**Caracter√≠sticas clave:**
- Verificaci√≥n en dos pasos (metadata + prueba real)
- Sistema de logging con iconos visuales (‚úÖ/‚ùå)
- Optimizado para rendimiento (limit=100, delays controlados)
- Filtrado por modelos populares y especializados

Este script sirve como base para identificar modelos viables para experimentos comparativos en sistemas de debate entre LLMs, permitiendo filtrar solo aquellos con endpoints operativos.

"""

# --- Generaci√≥n de la S√≠ntesis del Moderador ---
print("\n=== GENERANDO RESUMEN DEL MODERADOR ===")

summary = "Error: El moderador no pudo generar un resumen." # Valor por defecto

# --- Recordatorio del Modelo Moderador Actual (Celda 8) ---
# Aseg√∫rate de que en Celda 8 est√© activo el modelo que quieres usar
# Ejemplo: moderator_model_id = "mistralai/Mistral-7B-Instruct-v0.3"
try:
    print(f"Intentando generar resumen con el moderador: {moderator_model_id}")
except NameError:
    print("ERROR: 'moderator_model_id' no est√° definido. Ejecuta Celda 8.")
    # Considera a√±adir sys.exit() si quieres detener aqu√≠

if 'moderator_client' not in locals() or moderator_client is None:
     print("ERROR: 'moderator_client' no est√° inicializado. Ejecuta Celda 8.")
     # Considera a√±adir sys.exit() aqu√≠

elif responses:
    # 1. Recopilar texto de las respuestas v√°lidas (limpiadas en Celda 5)
    texto_respuestas_completas = ""
    valid_response_count = 0
    for name, resp in responses.items(): # 'responses' ya contiene las respuestas limpiadas/truncadas
         if resp and not resp.startswith("Error:"):
             valid_response_count += 1
             role_desc_context = name
             if name in available_models and 'original_role' in available_models[name]:
                 role_desc_context = f"{name} ({available_models[name]['original_role']})"
             texto_respuestas_completas += f"--- Perspectiva de {role_desc_context} ---\n{resp}\n\n"

    if valid_response_count == 0:
        print("‚ö†Ô∏è No hay respuestas v√°lidas para que el moderador resuma.")
        summary = "Error: No se generaron respuestas v√°lidas en el debate para resumir."
    else:
        # 2. Construir el prompt para el moderador (v4: Dr√°sticamente Simplificado)
        max_context_length = 6000 # Ajusta si es necesario
        contexto_limitado = texto_respuestas_completas[:max_context_length]
        if len(texto_respuestas_completas) > max_context_length:
             contexto_limitado += "\n... (contenido truncado)\n"

        # v4: Prompt MUY simple, eliminando la estructura forzada de 4 partes.
        moderador_prompt = f"""**Tarea del Moderador Experto:**
Eres un moderador de debates. Has observado una discusi√≥n sobre "{topic}". A continuaci√≥n se presentan las perspectivas de los participantes y un an√°lisis de similitud.

SINTETIZA la discusi√≥n de manera **neutral y objetiva en uno o m√°s p√°rrafos coherentes**.
Tu resumen debe cubrir los **argumentos principales** presentados, las √°reas donde hubo cierto **acuerdo** (aunque sea en reconocer el problema) y los puntos clave de **desacuerdo** o las **perspectivas contrastantes**. Concluye con una visi√≥n general del estado del debate.
NO a√±adas tu opini√≥n personal. Basa tu resumen √∫nicamente en el contenido proporcionado.

**S√≠ntesis del Debate:**
""" # FIN DEL PROMPT SIMPLIFICADO

        # 3. Llamar al modelo moderador con reintentos (Manteniendo Temp 0.6)
        print(f"‚è≥ Generando s√≠ntesis con {moderator_model_id} (prompt v4 - simple, temp=0.6)...")
        summary = generate_with_retry(
            moderator_client,
            moderador_prompt,
            max_retries=2,
            wait_time=10,
            max_new_tokens=1500, # Mantener espacio para un resumen completo
            temperature=0.6,   # Mantener temperatura moderada
            stop_sequences=["\n\n**", "\n---", "==="],
            do_sample=True
        )

        # 4. Procesamiento y muestra de resultados (sin cambios significativos aqu√≠)
        if not summary.startswith("Error:") and summary.strip() and summary.strip() != "---":
             print("‚úÖ Resumen del moderador generado.")
             if summary.strip().startswith("S√≠ntesis del Debate"):
                 summary = summary.split("\n", 1)[-1]
             if "**S√≠ntesis del Debate" in summary:
                 summary = summary.split("**S√≠ntesis del Debate")[0]

        elif summary.strip() == "---" or not summary.strip():
             print("‚ùå Error: El moderador gener√≥ una respuesta vac√≠a o solo separadores ('---').")
             summary = "Error: El moderador no gener√≥ contenido v√°lido."
        else:
            print(f"‚ùå Error al generar el resumen del moderador: {summary}")

    print(f"\n--- üî∏ Moderador (S√≠ntesis del Debate) üî∏ ---\n")
    if isinstance(summary, str):
      print(summary.strip())
    else:
      print(f"Error: El tipo de 'summary' no es string ({type(summary)}). Valor: {summary}")
    print("\n--------------------------------------------")

elif 'moderator_client' in locals() and moderator_client is None:
    print("ERROR: El cliente del moderador no se inicializ√≥ correctamente en Celda 8.")
elif not responses:
    print("ERROR: No hay respuestas del debate para resumir (variable 'responses' vac√≠a).")

"""# Guardado de resultados"""

# --- Guardar resultados en un archivo .txt ---
print("\n=== GUARDANDO RESULTADOS COMPLETOS ===")

# (Nombre de archivo din√°mico como antes)
timestamp = time.strftime('%Y%m%d_%H%M%S')
safe_topic = re.sub(r'[^\w\-]+', '_', topic.lower()).strip('_')
output_filename = f"debate_con_gdn_{safe_topic[:30]}_{timestamp}.txt"

try:
    with open(output_filename, "w", encoding="utf-8") as f:
        f.write(f"=== DEBATE SIMULADO CON AN√ÅLISIS GDN ===\n")
        f.write(f"TEMA: {topic}\n")
        f.write(f"Fecha y Hora: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*70 + "\n\n")

        # --- PERSPECTIVAS INDIVIDUALES (Como antes) ---
        f.write("--- PERSPECTIVAS INDIVIDUALES ---\n\n")
        if responses:
             for model_name_file, response_file in responses.items():
                 # ... (c√≥digo existente para escribir respuestas) ...
                  role_desc_for_file = model_name_file
                  if model_name_file in available_models and 'original_role' in available_models[model_name_file]:
                       role_desc_for_file = f"{model_name_file} ({available_models[model_name_file]['original_role']})"
                  f.write(f"--- {role_desc_for_file} ---\n")
                  f.write(f"{response_file}\n\n")
        else:
            f.write("No se generaron respuestas individuales.\n\n")
        f.write("="*70 + "\n\n")

        # --- AN√ÅLISIS DE SIMILITUD (Como antes) ---
        f.write(f"--- AN√ÅLISIS DE SIMILITUD SEM√ÅNTICA (Umbral > {similarity_threshold:.2f}) ---\n")
        f.write(analysis_summary) # Escribir el resumen del an√°lisis
        f.write("\n\n" + "="*70 + "\n\n")

        # --- NUEVO: ARGUMENTOS CLAVE EXTRA√çDOS ---
        f.write("--- ARGUMENTOS CLAVE EXTRA√çDOS PARA EVALUACI√ìN ---\n\n")
        if 'arguments_for_evaluation' in locals() and arguments_for_evaluation:
            for arg_id, arg_text in arguments_for_evaluation.items():
                 source_model = argument_source.get(arg_text, "M√∫ltiples?")
                 f.write(f"ID: {arg_id}\n")
                 f.write(f"Origen Aproximado: {source_model}\n")
                 f.write(f"Texto: {arg_text}\n\n")
        else:
            f.write("No se extrajeron argumentos clave.\n\n")
        f.write("="*70 + "\n\n")

        # --- NUEVO: PREFERENCIAS INDIVIDUALES ---
        f.write("--- PREFERENCIAS INDIVIDUALES SIMULADAS (Puntuaciones 1-10) ---\n\n")
        if 'individual_preferences' in locals() and individual_preferences:
             f.write("Argumento ID -> Puntuaci√≥n por Modelo\n")
             # Escribir tabla o lista de preferencias
             # Encabezados
             model_names = list(individual_preferences.keys())
             header = "Argumento ID      | " + " | ".join([f"{name[:10]:^10}" for name in model_names]) + "\n"
             f.write(header)
             f.write("-" * len(header) + "\n")
             # Filas
             for arg_id in arguments_for_evaluation.keys():
                 ratings_row = [f"{individual_preferences[m].get(arg_id, '-'):^10}" for m in model_names]
                 f.write(f"{arg_id[:15]:<15} | " + " | ".join(ratings_row) + "\n")
             f.write("\n")
        else:
             f.write("No se generaron preferencias individuales.\n\n")
        f.write("="*70 + "\n\n")

        # --- NUEVO: CONSENSO Y PREFERENCIA COLECTIVA ---
        f.write("--- CONSENSO Y PREFERENCIA COLECTIVA ---\n\n")
        f.write(f"Nivel de Consenso Calculado: {consensus_level:.4f}\n")
        f.write(f"Detalles del C√°lculo: {consensus_details}\n")
        f.write(f"Umbral de Consenso Requerido: {consensus_threshold}\n\n")
        f.write("Preferencia Colectiva (Puntuaci√≥n Promedio por Argumento):\n")
        if 'collective_preference' in locals() and collective_preference:
             # Usar el ranking ordenado si existe
             sorted_pref = final_ranking_tuples if 'final_ranking_tuples' in locals() else sorted(collective_preference.items(), key=lambda item: item[1], reverse=True)
             for arg_id, avg_rating in sorted_pref:
                  f.write(f"  - Rating: {avg_rating:.3f} | ID: {arg_id[:15]} | Texto: {arguments_for_evaluation[arg_id][:100]}...\n")
        else:
            f.write("  No calculada.\n")
        f.write("\n" + "="*70 + "\n\n")

        # --- MODERADOR (S√çNTESIS - Como antes, pero quiz√°s informada por el consenso) ---
        # Nota: Podr√≠as modificar el prompt del moderador (Celda 9) para incluir
        #       el resultado del consenso y el ranking si lo deseas.
        f.write("--- MODERADOR (S√çNTESIS DEL DEBATE) ---\n\n")
        f.write(summary) # Escribir el resumen del moderador
        f.write("\n\n" + "="*70 + "\n\n")

        # --- NUEVO: RANKING FINAL O ESTADO DE NEGOCIACI√ìN ---
        f.write("--- RESULTADO FINAL DEL PROCESO GDN ---\n\n")
        if isinstance(final_ranking, str): # Si es un mensaje (ej. "Negociaci√≥n requerida...")
            f.write(f"Estado: {final_ranking}\n")
            if needs_negotiation:
                # Podr√≠as listar los puntos de desacuerdo aqu√≠ tambi√©n
                 f.write("\nPuntos clave de desacuerdo identificados:\n")
                 # Reutilizar la l√≥gica de Celda 5.4 para mostrar puntos de desacuerdo
                 if 'disagreement_points' in locals():
                      for i, point in enumerate(disagreement_points[:5]):
                           f.write(f"  {i+1}. StdDev: {point['std_dev']:.2f} | ID: {point['id'][:15]} | Texto: {arguments_for_evaluation[point['id']][:90]}...\n")

        elif final_ranking: # Si es una lista (el ranking)
            f.write("Ranking Final de Argumentos (Consenso Alcanzado):\n")
            f.write("-" * 60 + "\n")
            f.write(f"{'Rank':<5} | {'Avg Rating':<12} | {'Argumento (inicio)':<80} | {'Individual Ratings'}\n")
            f.write("-" * 140 + "\n")
            for item in final_ranking:
                 ratings_str = ", ".join([f"{m[:5]}:{r}" for m, r in item["individual_ratings"].items()])
                 f.write(f"{item['rank']:<5} | {item['avg_rating']:<12.3f} | {item['text'][:80]:<80} | {ratings_str}\n")
        else:
            f.write("No se determin√≥ un ranking final.\n")
        f.write("\n" + "="*70 + "\n")
        f.write("--- FIN DEL REPORTE ---")

    print(f"‚úÖ Resultados completos (incluyendo GDN) guardados en: '{output_filename}'")
    print(" ¬† Puedes encontrar este archivo en el panel izquierdo de Colab.")

except Exception as e:
    print(f"‚ùå Error al guardar el archivo '{output_filename}': {str(e)}")


print("\n=== PROCESO COMPLETO (DEBATE + GDN) FINALIZADO ===")

print(arguments_for_evaluation)

import pandas as pd
import numpy as np
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util
from sklearn.metrics.pairwise import cosine_similarity

# --- 1. MODELOS MULTILING√úES ---
llm = pipeline("zero-shot-classification", model="Recognai/zeroshot_selectra_medium")
embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# --- 2. PROMPTS OPTIMIZADOS ---
plantillas = {
    "claridad": (
        "Eval√∫a claridad del argumento (1-5):\n"
        "1: Incomprensible\n3: Comprensible con esfuerzo\n5: Extremadamente claro\n\n"
        "Argumento: {texto}\nCalificaci√≥n:"
    ),
    "coherencia": (
        "Eval√∫a coherencia l√≥gica (1-5):\n"
        "1: Contradictorio\n3: Parcialmente coherente\n5: Perfectamente estructurado\n\n"
        "Argumento: {texto}\nCalificaci√≥n:"
    ),
    "neutralidad": (
        "Eval√∫a neutralidad (1-5):\n"
        "1: Claramente sesgado\n3: Neutral con alg√∫n sesgo\n5: Completamente objetivo\n\n"
        "Argumento: {texto}\nCalificaci√≥n:"
    )
}

# --- 3. FUNCI√ìN DE EVALUACI√ìN ROBUSTA ---
def evalua_dimension_llm(texto, dimension):
    prompt = plantillas[dimension].format(texto=texto)
    result = llm(prompt, candidate_labels=["1", "2", "3", "4", "5"], hypothesis_template="{}")
    return int(result['labels'][0])  # Toma la etiqueta con mayor score

# --- 4. COBERTURA TEM√ÅTICA MEJORADA ---
subtemas = ["impacto social", "√©tica", "tecnolog√≠a", "sociedad", "regulaci√≥n"]
subtema_embs = embedder.encode(subtemas)

def cobertura_semantica(texto):
    arg_emb = embedder.encode([texto])
    sims = cosine_similarity(arg_emb, subtema_embs)[0]
    return np.max(sims)  # Devuelve valor continuo

# --- 5. DIVERSIDAD CON PERCENTIL ---
def calcular_diversidad(textos):
    embs = embedder.encode(textos)
    sim_matrix = cosine_similarity(embs)
    np.fill_diagonal(sim_matrix, -1)  # Ignorar diagonal
    return 1 - np.percentile(sim_matrix, 95)  # Percentil 95

# --- 6. EJECUCI√ìN PRINCIPAL ---
# (Mantener estructura de DataFrames pero con m√©tricas mejoradas)
df_stats["cobertura"] = df_argumentos["texto"].apply(cobertura_semantica)
diversidad_val = calcular_diversidad(df_argumentos["texto"].tolist())

# --- 7. M√âTRICAS GLOBALES CON INTERPRETACI√ìN ---
print(f"Diversidad sem√°ntica (P95): {diversidad_val:.2f}")
print(f"Cobertura tem√°tica promedio: {df_stats['cobertura'].mean():.2f}/1.0")

"""- Una diversidad sem√°ntica de 0.41 (en una escala donde 1 es m√°xima diversidad y 0 es m√≠nima) significa que, aunque hay diferencias entre los argumentos, existe una cantidad considerable de similitud entre ellos. En otras palabras, muchos argumentos comparten ideas, enfoques o vocabulario, y solo una minor√≠a se aleja realmente del resto.

- El percentil 95 se utiliza para que la m√©trica no se vea afectada por unos pocos pares extremadamente similares (outliers), proporcionando as√≠ una visi√≥n m√°s robusta de la diversidad real en el conjunto.

- Un valor de 0.36 en cobertura tem√°tica (donde 1.0 ser√≠a que todos los argumentos cubren perfectamente alg√∫n subtema relevante) indica que la mayor√≠a de los argumentos no abordan directamente los subtemas clave definidos (por ejemplo: "impacto social", "√©tica", "tecnolog√≠a", etc.). Solo un tercio de los argumentos, aproximadamente, logra conectar sem√°nticamente con estos subtemas.
"""

import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.barh(df_stats["argument_id"], df_stats["cobertura"])
plt.title("Cobertura Tem√°tica por Argumento")
plt.show()

from sklearn.feature_extraction.text import CountVectorizer

# --- 1. Preparar DataFrames ---
df_argumentos = pd.DataFrame([
    {"argument_id": arg_id, "texto": data, "origen": "unknown"}
    for arg_id, data in arguments_for_evaluation.items()
])

df_prefs = pd.DataFrame(individual_preferences).T
df_stats = pd.DataFrame({
    "argument_id": df_prefs.columns,
    "media": df_prefs.mean(),
    "desviacion": df_prefs.std()
}).reset_index(drop=True)

# --- 2. Ranking por preferencia colectiva ---
df_stats["preferencia_colectiva"] = df_stats["media"]
df_stats_sorted = df_stats.sort_values(by="preferencia_colectiva", ascending=False)

columnas_resumen = [
    "argument_id", "media", "desviacion", "preferencia_colectiva"
]
# A√±ade aqu√≠ las columnas de claridad, coherencia, neutralidad, cobertura si las tienes calculadas

df_resumen = df_stats_sorted.merge(df_argumentos, on="argument_id")
display(df_resumen[columnas_resumen + ["texto", "origen"]])

# --- 3. Visualizaci√≥n de preferencias individuales y colectivas ---
media_individual = df_prefs.mean(axis=1)
media_colectiva = df_stats["media"].mean()

plt.figure(figsize=(8,6))
plt.scatter(range(len(media_individual)), media_individual, label="Preferencia individual", alpha=0.7)
plt.axhline(media_colectiva, color='red', linestyle='--', label="Media colectiva")
plt.xlabel("Usuarios")
plt.ylabel("Puntuaci√≥n media")
plt.title("Preferencias individuales vs preferencia colectiva")
plt.legend()
plt.grid(True)
plt.show()

# --- 4. Redundancia sem√°ntica (similitud de argumentos) ---
from sentence_transformers import SentenceTransformer, util
from sklearn.metrics.pairwise import cosine_similarity

embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
arg_embs = embedder.encode(df_argumentos["texto"].tolist())
sim_matrix = cosine_similarity(arg_embs)
np.fill_diagonal(sim_matrix, 0)
umbral_redundancia = 0.8
redundancias = []

for i in range(len(sim_matrix)):
    for j in range(i+1, len(sim_matrix)):
        if sim_matrix[i, j] > umbral_redundancia:
            redundancias.append((df_argumentos.iloc[i]["argument_id"],
                                 df_argumentos.iloc[j]["argument_id"],
                                 sim_matrix[i, j]))

print(f"\nN√∫mero de pares redundantes (> {umbral_redundancia} similitud): {len(redundancias)}")
if redundancias:
    print("Pares de argumentos redundantes:")
    for a, b, sim in redundancias:
        print(f" - {a} y {b} (similitud: {sim:.2f})")

# --- 5. Nivel de consenso (desviaci√≥n est√°ndar normalizada) ---
max_std = 4.5  # M√°ximo te√≥rico para escala 1-10
nivel_consenso = 1 - (df_stats["desviacion"].mean() / max_std)
print(f"Nivel de consenso estimado (1=alto, 0=bajo): {nivel_consenso:.2f}")

# --- 6. Frases clave frecuentes (stopwords en espa√±ol) ---
spanish_stop_words = [
    "de", "la", "que", "el", "en", "y", "a", "los", "del", "se",
    "las", "por", "un", "para", "con", "no", "una", "su", "al", "es", "lo", "como", "m√°s", "pero", "sus", "le"
]

vectorizer = CountVectorizer(
    ngram_range=(2, 4),
    stop_words=spanish_stop_words
).fit(df_argumentos["texto"])

counts = vectorizer.transform(df_argumentos["texto"])
sum_words = counts.sum(axis=0)
keywords_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
keywords_freq = sorted(keywords_freq, key=lambda x: x[1], reverse=True)[:10]

print("\nFrases clave m√°s frecuentes:")
for kw, freq in keywords_freq:
    print(f" - {kw} ({freq})")

# --- 7. Ranking final (puedes ponderar si tienes m√°s m√©tricas) ---
df_stats["ranking_final"] = df_stats["media"]  # O usa una combinaci√≥n ponderada
df_stats_final = df_stats.sort_values(by="ranking_final", ascending=False).reset_index(drop=True)

# --- 8. (Opcional) Mapa de calor de similitud sem√°ntica ---
import seaborn as sns
plt.figure(figsize=(10,8))
sns.heatmap(
    sim_matrix,
    annot=False,
    xticklabels=df_argumentos["argument_id"],
    yticklabels=df_argumentos["argument_id"]
)
plt.title("Matriz de Similitud Sem√°ntica entre Argumentos")
plt.show()

"""# UI"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import time
# import json
# import re
# import uuid
# import itertools
# import traceback
# from datetime import datetime
# import matplotlib.pyplot as plt
# import plotly.express as px
# import plotly.graph_objects as go
# 
# # Importaciones para an√°lisis sem√°ntico y LLMs
# from huggingface_hub import InferenceClient, login, list_models, model_info
# from sentence_transformers import SentenceTransformer, util
# from sklearn.metrics.pairwise import cosine_similarity
# from sklearn.feature_extraction.text import CountVectorizer
# from transformers import pipeline
# 
# # Configuraci√≥n de la p√°gina
# st.set_page_config(
#     page_title="Sistema de Debate Multi-Agente",
#     page_icon="ü§ñ",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )
# 
# st.title("ü§ñ Sistema de Debate Multi-Agente con LLMs")
# st.markdown("### An√°lisis automatizado de debates con Group Decision Making (GDN)")
# 
# # Inicializaci√≥n de variables de sesi√≥n
# if 'debate_completed' not in st.session_state:
#     st.session_state.debate_completed = False
# if 'responses' not in st.session_state:
#     st.session_state.responses = {}
# if 'arguments_extracted' not in st.session_state:
#     st.session_state.arguments_extracted = False
# if 'gdn_completed' not in st.session_state:
#     st.session_state.gdn_completed = False
# 
# # Funciones auxiliares
# @st.cache_resource
# def load_sentence_transformer():
#     """Carga el modelo de sentence transformers con cache"""
#     try:
#         return SentenceTransformer('paraphrase-MiniLM-L6-v2')
#     except:
#         return SentenceTransformer('paraphrase-albert-small-v2')
# 
# @st.cache_resource
# def load_zero_shot_classifier():
#     """Carga el clasificador zero-shot con cache"""
#     return pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
# 
# def generate_with_retry(client, prompt, max_retries=3, wait_time=5, **kwargs):
#     """Funci√≥n de generaci√≥n con reintentos"""
#     default_params = {"max_new_tokens": 1000, "temperature": 0.7, "do_sample": True}
#     generation_params = {**default_params, **kwargs}
# 
#     for attempt in range(1, max_retries + 1):
#         try:
#             response = client.text_generation(prompt, **generation_params)
#             return response.strip()
#         except Exception as e:
#             if attempt == max_retries:
#                 return f"Error tras {max_retries} intentos: {str(e)}"
#             time.sleep(wait_time)
# 
#     return "Error: Se agotaron los reintentos."
# 
# def extract_key_arguments(client, text, topic):
#     """Extrae argumentos clave del texto"""
#     prompt = f"""
# **Tarea:** Eres un analista experto. Lee el siguiente texto sobre "{topic}".
# Extrae los **argumentos o puntos clave principales** presentados en el texto.
# Enumera cada argumento de forma concisa en una l√≠nea separada, empezando con '- '.
# 
# **Texto a analizar:**
# {text[:3000]}
# 
# **Argumentos Clave:**
# - """
# 
#     raw_extraction = generate_with_retry(client, prompt, max_new_tokens=500, temperature=0.3)
# 
#     if raw_extraction.startswith("Error:"):
#         return []
# 
#     extracted_points = []
#     lines = raw_extraction.strip().split('\n')
# 
#     for line in lines:
#         clean_line = line.strip()
#         if clean_line.startswith('- '):
#             point = clean_line[2:].strip()
#         else:
#             point = clean_line.strip()
# 
#         if len(point) > 20:
#             extracted_points.append(point)
# 
#     return extracted_points
# 
# def owa_aggregate(values, weights):
#     """Aplica agregaci√≥n OWA"""
#     values_sorted = sorted(values, reverse=True)
#     return sum(w * v for w, v in zip(weights, values_sorted))
# 
# def generate_owa_weights(n, alpha=1.0):
#     """Genera pesos OWA"""
#     ranks = np.arange(1, n + 1)
#     weights = (n - ranks + 1) ** alpha
#     weights /= weights.sum()
#     return weights
# 
# # Sidebar - Configuraci√≥n
# with st.sidebar:
#     st.header("üîß Configuraci√≥n")
# 
#     # Token de Hugging Face
#     hf_token = st.text_input(
#         "Token de Hugging Face",
#         type="password",
#         help="Obt√©n tu token en https://huggingface.co/settings/tokens"
#     )
# 
#     if hf_token and not hf_token.startswith("hf_"):
#         st.error("El token debe comenzar con 'hf_'")
# 
#     # Inicializar sesi√≥n HF
#     if hf_token and hf_token.startswith("hf_"):
#         try:
#             login(token=hf_token)
#             st.success("‚úÖ Conectado a Hugging Face")
#         except Exception as e:
#             st.error(f"Error de autenticaci√≥n: {str(e)}")
# 
# # Secci√≥n 1: Configuraci√≥n del Debate
# st.header("1. üìã Configuraci√≥n del Debate")
# 
# col1, col2 = st.columns([2, 1])
# 
# with col1:
#     topic = st.text_area(
#         "Tema del debate",
#         height=100,
#         placeholder="Ej: ¬øDeber√≠an regularse m√°s estrictamente las redes sociales?",
#         help="Ingresa el tema que quieres debatir"
#     )
# 
# with col2:
#     num_models = st.number_input(
#         "N√∫mero de modelos",
#         min_value=2,
#         max_value=5,
#         value=3,
#         help="Cantidad de LLMs que participar√°n en el debate"
#     )
# 
# # Lista de modelos disponibles (predefinida para simplificar)
# available_models = [
#     "microsoft/Phi-3-mini-4k-instruct",
#     "mistralai/Mistral-7B-Instruct-v0.3",
#     "meta-llama/Llama-2-7b-chat-hf",
#     "google/gemma-7b-it",
#     "HuggingFaceH4/zephyr-7b-beta",
#     "microsoft/Phi-3.5-mini-instruct",
#     "mistralai/Mixtral-8x7B-Instruct-v0.1"
# 
# ]
# 
# roles = ["Pro", "Contra", "Neutral"]
# 
# # Configuraci√≥n de modelos
# st.subheader("Selecci√≥n de Modelos y Roles")
# models_config = {}
# 
# for i in range(num_models):
#     col1, col2 = st.columns(2)
#     with col1:
#         selected_model = st.selectbox(
#             f"Modelo {i+1}",
#             available_models,
#             key=f"model_{i}"
#         )
#     with col2:
#         selected_role = st.selectbox(
#             f"Rol {i+1}",
#             roles,
#             key=f"role_{i}"
#         )
# 
#     models_config[f"Modelo_{i+1}"] = {
#         "model": selected_model,
#         "role": selected_role
#     }
# 
# # Secci√≥n 2: Generaci√≥n del Debate
# st.header("2. üé≠ Generaci√≥n del Debate")
# 
# if st.button("üöÄ Iniciar Debate", type="primary", disabled=not topic or not hf_token):
#     if not topic:
#         st.error("Por favor, ingresa un tema para el debate")
#     elif not hf_token:
#         st.error("Por favor, ingresa tu token de Hugging Face")
#     else:
#         # Inicializar clientes
#         progress_bar = st.progress(0)
#         status_text = st.empty()
# 
#         clients = {}
#         available_models_dict = {}
# 
#         for i, (model_name, config) in enumerate(models_config.items()):
#             status_text.text(f"Inicializando {model_name}...")
#             try:
#                 clients[model_name] = InferenceClient(provider="hf-inference", model=config["model"], token=hf_token)
#                 available_models_dict[model_name] = config
#                 st.success(f"‚úÖ {model_name} inicializado")
#             except Exception as e:
#                 st.error(f"‚ùå Error inicializando {model_name}: {str(e)}")
# 
#             progress_bar.progress((i + 1) / len(models_config))
# 
#         if clients:
#             st.session_state.clients = clients
#             st.session_state.available_models = available_models_dict
# 
#             # Plantillas de prompts
#             prompts = {
#                 "Pro": f"""**Tema del Debate:** {topic}
# **Tu Rol:** Participante experto **A FAVOR** de la premisa.
# **Tarea:** Exp√≥n argumentos clave A FAVOR. Fundamenta por qu√© es correcta, beneficiosa o necesaria.
# **Desarrollo:**""",
# 
#                 "Contra": f"""**Tema del Debate:** {topic}
# **Tu Rol:** Participante experto **EN CONTRA** de la premisa.
# **Tarea:** Exp√≥n argumentos clave EN CONTRA. Fundamenta riesgos, desventajas o problemas.
# **Desarrollo:**""",
# 
#                 "Neutral": f"""**Tema del Debate:** {topic}
# **Tu Rol:** Observador neutral que analiza ambas perspectivas.
# **Tarea:** Lista argumentos a favor y en contra del tema usando formato:
# - Argumento a favor
# - Argumento en contra
# **Lista:**"""
#             }
# 
#             # Generar respuestas
#             responses = {}
#             for model_name, config in available_models_dict.items():
#                 status_text.text(f"Generando respuesta de {model_name}...")
# 
#                 role = config.get('role', 'Pro')
#                 prompt = prompts.get(role, prompts['Pro'])
# 
#                 client = clients[model_name]
#                 response = generate_with_retry(
#                     client,
#                     prompt,
#                     temperature=0.7,
#                     max_new_tokens=1024
#                 )
# 
#                 responses[model_name] = response
# 
#                 # Mostrar respuesta
#                 with st.expander(f"üí¨ Respuesta de {model_name} ({role})"):
#                     st.write(response)
# 
#             st.session_state.responses = responses
#             st.session_state.debate_completed = True
#             status_text.text("‚úÖ Debate completado")
#             progress_bar.progress(1.0)
# 
# # Secci√≥n 3: Extracci√≥n de Argumentos
# if st.session_state.debate_completed:
#     st.header("3. üéØ Extracci√≥n de Argumentos Clave")
# 
#     if st.button("üìù Extraer Argumentos"):
#         with st.spinner("Extrayendo argumentos clave..."):
#             all_arguments = []
#             argument_source = {}
# 
#             for model_name, response in st.session_state.responses.items():
#                 if response and not response.startswith("Error:"):
#                     client = st.session_state.clients[model_name]
#                     args = extract_key_arguments(client, response, topic)
# 
#                     for arg in args:
#                         all_arguments.append(arg)
#                         argument_source[arg] = model_name
# 
#             # Deduplicaci√≥n sem√°ntica
#             embedder = load_sentence_transformer()
# 
#             if len(all_arguments) > 1:
#                 embeddings = embedder.encode(all_arguments)
#                 similarity_matrix = cosine_similarity(embeddings)
# 
#                 unique_arguments = []
#                 used = set()
# 
#                 for i, arg in enumerate(all_arguments):
#                     if i not in used:
#                         unique_arguments.append(arg)
#                         for j in range(i + 1, len(all_arguments)):
#                             if similarity_matrix[i][j] > 0.85:
#                                 used.add(j)
#             else:
#                 unique_arguments = all_arguments
# 
#             # Crear diccionario de argumentos √∫nicos
#             arguments_for_evaluation = {
#                 str(uuid.uuid4()): arg_text
#                 for arg_text in unique_arguments
#             }
# 
#             st.session_state.arguments_for_evaluation = arguments_for_evaluation
#             st.session_state.argument_source = argument_source
#             st.session_state.arguments_extracted = True
# 
#             # Mostrar argumentos extra√≠dos
#             st.subheader(f"Argumentos √önicos Extra√≠dos ({len(unique_arguments)})")
# 
#             df_args = pd.DataFrame([
#                 {
#                     "ID": arg_id[:8],
#                     "Argumento": arg_text,
#                     "Origen": argument_source.get(arg_text, "M√∫ltiple")
#                 }
#                 for arg_id, arg_text in arguments_for_evaluation.items()
#             ])
# 
#             st.dataframe(df_args, use_container_width=True)
# 
# # Secci√≥n 4: An√°lisis GDN
# if st.session_state.arguments_extracted:
#     st.header("4. üé≤ An√°lisis Group Decision Making (GDN)")
# 
#     if st.button("üîç Evaluar Argumentos con GDN"):
#         with st.spinner("Evaluando argumentos..."):
#             individual_preferences = {}
# 
#             # Evaluar cada argumento con cada modelo
#             for model_name, client in st.session_state.clients.items():
#                 ratings = {}
# 
#                 for arg_id, argument in st.session_state.arguments_for_evaluation.items():
#                     prompt = f"""ROL: Eres '{model_name}' evaluando argumentos sobre '{topic}'
# TAREA: Califica este argumento (1-10):
# {argument}
# 
# Criterios:
# 1. Claridad: ¬øEs comprensible?
# 2. Coherencia l√≥gica: ¬øEs v√°lido?
# 3. Relevancia: ¬øSe relaciona con el tema?
# 
# Solo responde con un n√∫mero del 1 al 10:"""
# 
#                     response = generate_with_retry(
#                         client,
#                         prompt,
#                         max_new_tokens=15,
#                         temperature=0.3
#                     )
# 
#                     # Extraer puntuaci√≥n
#                     match = re.search(r'\b(10|\d)\b', response)
#                     if match:
#                         score = int(match.group())
#                         ratings[arg_id] = max(1, min(10, score))
#                     else:
#                         ratings[arg_id] = 5  # Valor por defecto
# 
#                 individual_preferences[model_name] = ratings
# 
#             st.session_state.individual_preferences = individual_preferences
# 
#             # Calcular preferencia colectiva con OWA
#             num_voters = len(individual_preferences)
#             owa_weights = generate_owa_weights(num_voters, alpha=1.0)
# 
#             collective_preference = {}
#             for arg_id in st.session_state.arguments_for_evaluation.keys():
#                 ratings = [prefs.get(arg_id, 5) for prefs in individual_preferences.values()]
#                 agg_value = owa_aggregate(ratings, owa_weights)
#                 collective_preference[arg_id] = agg_value
# 
#             # Calcular consenso
#             consensus_diffs = []
#             for arg_id in st.session_state.arguments_for_evaluation.keys():
#                 ratings = [prefs.get(arg_id, 5) for prefs in individual_preferences.values()]
#                 owa_val = collective_preference[arg_id]
#                 var = np.mean([(r - owa_val) ** 2 for r in ratings])
#                 consensus_diffs.append(var)
# 
#             avg_var = np.mean(consensus_diffs)
#             consensus_level = round(1.0 - (avg_var / 20.25), 3)
# 
#             st.session_state.collective_preference = collective_preference
#             st.session_state.consensus_level = consensus_level
#             st.session_state.gdn_completed = True
# 
#             # Mostrar resultados
#             st.subheader("üìä Resultados del An√°lisis GDN")
# 
#             col1, col2 = st.columns(2)
# 
#             with col1:
#                 st.metric("Nivel de Consenso", f"{consensus_level:.3f}")
#                 st.metric("N√∫mero de Argumentos", len(collective_preference))
# 
#             with col2:
#                 st.metric("Promedio de Puntuaciones", f"{np.mean(list(collective_preference.values())):.2f}")
#                 st.metric("Participantes", len(individual_preferences))
# 
#             # Ranking de argumentos
#             sorted_collective = sorted(
#                 collective_preference.items(),
#                 key=lambda x: x[1],
#                 reverse=True
#             )
# 
#             st.subheader("üèÜ Ranking de Argumentos")
# 
#             ranking_data = []
#             for i, (arg_id, score) in enumerate(sorted_collective):
#                 argument_text = st.session_state.arguments_for_evaluation[arg_id]
#                 individual_scores = [
#                     individual_preferences[model].get(arg_id, 'N/A')
#                     for model in individual_preferences.keys()
#                 ]
# 
#                 ranking_data.append({
#                     "Rank": i + 1,
#                     "Puntuaci√≥n": f"{score:.2f}",
#                     "Argumento": argument_text[:100] + "..." if len(argument_text) > 100 else argument_text,
#                     "Puntuaciones Individuales": ", ".join(map(str, individual_scores))
#                 })
# 
#             df_ranking = pd.DataFrame(ranking_data)
#             st.dataframe(df_ranking, use_container_width=True)
# 
# # Secci√≥n 5: An√°lisis Sem√°ntico
# if st.session_state.gdn_completed:
#     st.header("5. üß† An√°lisis Sem√°ntico")
# 
#     if st.button("üîç Analizar Similitudes Sem√°nticas"):
#         embedder = load_sentence_transformer()
# 
#         # An√°lisis de similitudes entre respuestas
#         valid_responses = {
#             name: resp for name, resp in st.session_state.responses.items()
#             if resp and not resp.startswith("Error:")
#         }
# 
#         if len(valid_responses) >= 2:
#             similarity_results = []
# 
#             for model1, model2 in itertools.combinations(valid_responses.keys(), 2):
#                 text1 = valid_responses[model1]
#                 text2 = valid_responses[model2]
# 
#                 # Dividir en frases
#                 sentences1 = re.split(r'(?<=[.!?])\s+', text1.strip())
#                 sentences2 = re.split(r'(?<=[.!?])\s+', text2.strip())
# 
#                 # Filtrar frases cortas
#                 sentences1 = [s.strip() for s in sentences1 if len(s.strip()) > 20]
#                 sentences2 = [s.strip() for s in sentences2 if len(s.strip()) > 20]
# 
#                 if sentences1 and sentences2:
#                     # Calcular embeddings
#                     emb1 = embedder.encode(sentences1)
#                     emb2 = embedder.encode(sentences2)
# 
#                     # Similitud coseno
#                     sim_matrix = cosine_similarity(emb1, emb2)
#                     max_sim = np.max(sim_matrix)
#                     avg_sim = np.mean(sim_matrix)
# 
#                     similarity_results.append({
#                         "Par de Modelos": f"{model1} vs {model2}",
#                         "Similitud M√°xima": f"{max_sim:.3f}",
#                         "Similitud Promedio": f"{avg_sim:.3f}"
#                     })
# 
#             if similarity_results:
#                 df_similarity = pd.DataFrame(similarity_results)
#                 st.dataframe(df_similarity, use_container_width=True)
# 
#         # An√°lisis de diversidad sem√°ntica
#         if st.session_state.arguments_extracted:
#             arguments_list = list(st.session_state.arguments_for_evaluation.values())
#             arg_embeddings = embedder.encode(arguments_list)
#             arg_similarity = cosine_similarity(arg_embeddings)
# 
#             # Llenar diagonal con 0
#             np.fill_diagonal(arg_similarity, 0)
#             diversity_score = 1 - np.mean(arg_similarity)
# 
#             st.metric("Diversidad Sem√°ntica Global", f"{diversity_score:.3f}")
# 
# # Secci√≥n 6: M√©tricas Avanzadas
# if st.session_state.gdn_completed:
#     st.header("6. üìà M√©tricas Avanzadas")
# 
#     if st.button("üìä Calcular M√©tricas Avanzadas"):
#         # Cargar clasificador zero-shot
#         classifier = load_zero_shot_classifier()
# 
#         def evaluate_dimension(text, dimension):
#             if dimension == "claridad":
#                 prompt = f"Este argumento es claro y comprensible: {text[:500]}"
#                 labels = ["muy poco claro", "poco claro", "moderadamente claro", "claro", "muy claro"]
#             elif dimension == "coherencia":
#                 prompt = f"Este argumento es l√≥gicamente coherente: {text[:500]}"
#                 labels = ["incoherente", "poco coherente", "moderadamente coherente", "coherente", "muy coherente"]
#             else:  # neutralidad
#                 prompt = f"Este argumento es neutral y objetivo: {text[:500]}"
#                 labels = ["muy sesgado", "sesgado", "moderadamente neutral", "neutral", "muy neutral"]
# 
#             result = classifier(prompt, candidate_labels=labels)
#             return labels.index(result['labels'][0]) + 1
# 
#         # Evaluar argumentos
#         metrics_data = []
# 
#         for arg_id, arg_text in st.session_state.arguments_for_evaluation.items():
#             claridad = evaluate_dimension(arg_text, "claridad")
#             coherencia = evaluate_dimension(arg_text, "coherencia")
#             neutralidad = evaluate_dimension(arg_text, "neutralidad")
# 
#             # Obtener puntuaci√≥n colectiva
#             collective_score = st.session_state.collective_preference.get(arg_id, 0)
# 
#             metrics_data.append({
#                 "ID": arg_id[:8],
#                 "Argumento": arg_text[:80] + "..." if len(arg_text) > 80 else arg_text,
#                 "Claridad": claridad,
#                 "Coherencia": coherencia,
#                 "Neutralidad": neutralidad,
#                 "Puntuaci√≥n Colectiva": f"{collective_score:.2f}"
#             })
# 
#         df_metrics = pd.DataFrame(metrics_data)
#         st.dataframe(df_metrics, use_container_width=True)
# 
#         # M√©tricas globales
#         st.subheader("üìã M√©tricas Globales")
# 
#         col1, col2, col3 = st.columns(3)
# 
#         with col1:
#             st.metric("Claridad Media", f"{df_metrics['Claridad'].mean():.2f} / 5")
#             st.metric("Coherencia Media", f"{df_metrics['Coherencia'].mean():.2f} / 5")
# 
#         with col2:
#             st.metric("Neutralidad Media", f"{df_metrics['Neutralidad'].mean():.2f} / 5")
#             st.metric("Nivel de Consenso", f"{st.session_state.consensus_level:.3f}")
# 
#         with col3:
#             avg_score = np.mean(list(st.session_state.collective_preference.values()))
#             st.metric("Puntuaci√≥n Media", f"{avg_score:.2f} / 10")
#             st.metric("Total Argumentos", len(st.session_state.arguments_for_evaluation))
# 
# # Secci√≥n 7: Visualizaciones
# if st.session_state.gdn_completed:
#     st.header("7. üìä Visualizaciones")
# 
#     tab1, tab2, tab3 = st.tabs(["Distribuci√≥n de Puntuaciones", "Consenso por Argumento", "Matriz de Similitud"])
# 
#     with tab1:
#         if st.session_state.individual_preferences:
#             # Gr√°fico de distribuci√≥n de puntuaciones
#             all_scores = []
#             models = []
# 
#             for model, preferences in st.session_state.individual_preferences.items():
#                 scores = list(preferences.values())
#                 all_scores.extend(scores)
#                 models.extend([model] * len(scores))
# 
#             fig = px.box(
#                 x=models,
#                 y=all_scores,
#                 title="Distribuci√≥n de Puntuaciones por Modelo",
#                 labels={"x": "Modelo", "y": "Puntuaci√≥n"}
#             )
#             st.plotly_chart(fig, use_container_width=True)
# 
#     with tab2:
#         if st.session_state.collective_preference:
#             # Gr√°fico de consenso
#             arg_ids = list(st.session_state.collective_preference.keys())
#             scores = list(st.session_state.collective_preference.values())
# 
#             # Calcular varianza para cada argumento
#             variances = []
#             for arg_id in arg_ids:
#                 ratings = [
#                     prefs.get(arg_id, 5)
#                     for prefs in st.session_state.individual_preferences.values()
#                 ]
#                 variances.append(np.var(ratings))
# 
#             fig = px.scatter(
#                 x=scores,
#                 y=variances,
#                 title="Puntuaci√≥n vs Varianza (Consenso)",
#                 labels={"x": "Puntuaci√≥n Colectiva", "y": "Varianza"},
#                 hover_data={"Argumento": [
#                     st.session_state.arguments_for_evaluation[arg_id][:50] + "..."
#                     for arg_id in arg_ids
#                 ]}
#             )
#             st.plotly_chart(fig, use_container_width=True)
# 
#     with tab3:
#         if st.session_state.individual_preferences:
#             # Matriz de correlaci√≥n entre modelos
#             df_prefs = pd.DataFrame(st.session_state.individual_preferences)
#             corr_matrix = df_prefs.corr()
# 
#             fig = px.imshow(
#                 corr_matrix,
#                 title="Matriz de Correlaci√≥n entre Modelos",
#                 color_continuous_scale="RdBu_r"
#             )
#             st.plotly_chart(fig, use_container_width=True)
# 
# # Secci√≥n 8: S√≠ntesis del Moderador
# if st.session_state.debate_completed:
#     st.header("8. üéØ S√≠ntesis del Moderador")
# 
#     if st.button("üìù Generar S√≠ntesis Final"):
#         # Usar uno de los modelos como moderador
#         moderator_model = list(st.session_state.clients.keys())[0]
#         moderator_client = st.session_state.clients[moderator_model]
# 
#         # Preparar contexto
#         context = f"Tema del debate: {topic}\n\n"
#         for model_name, response in st.session_state.responses.items():
#             if not response.startswith("Error:"):
#                 role = st.session_state.available_models[model_name]['role']
#                 context += f"--- {model_name} ({role}) ---\n{response}\n\n"
# 
#         # Prompt para s√≠ntesis
#         synthesis_prompt = f"""**Tarea del Moderador:**
# Sintetiza el siguiente debate sobre "{topic}" de manera neutral y objetiva.
# 
# {context[:6000]}
# 
# Proporciona un resumen que incluya:
# - Argumentos principales presentados
# - Puntos de acuerdo identificados
# - Principales diferencias de perspectiva
# - S√≠ntesis general del estado del debate
# 
# **S√≠ntesis:**"""
# 
#         with st.spinner("Generando s√≠ntesis..."):
#             synthesis = generate_with_retry(
#                 moderator_client,
#                 synthesis_prompt,
#                 max_new_tokens=1500,
#                 temperature=0.6
#             )
# 
#         st.subheader("üìã S√≠ntesis del Debate")
#         st.write(synthesis)
# 
#         st.session_state.synthesis = synthesis
# 
# # Secci√≥n 9: Exportaci√≥n de Resultados
# st.header("9. üíæ Exportaci√≥n de Resultados")
# 
# if st.session_state.debate_completed:
#     col1, col2 = st.columns(2)
# 
#     with col1:
#         if st.button("üìÑ Generar Reporte Completo"):
#             # Crear reporte en texto
#             timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
# 
#             report = f"""=== REPORTE DE DEBATE AUTOMATIZADO ===
# Tema: {topic}
# Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# {'='*70}
# 
# --- CONFIGURACI√ìN ---
# N√∫mero de modelos: {len(st.session_state.available_models)}
# Modelos utilizados:
# """
# 
#             for model_name, config in st.session_state.available_models.items():
#                 report += f"  - {model_name}: {config['model']} ({config['role']})\n"
# 
#             report += f"\n--- RESPUESTAS GENERADAS ---\n"
#             for model_name, response in st.session_state.responses.items():
#                 role = st.session_state.available_models[model_name]['role']
#                 report += f"\n{model_name} ({role}):\n{response}\n{'-'*50}\n"
# 
#             if hasattr(st.session_state, 'consensus_level'):
#                 report += f"\n--- AN√ÅLISIS GDN ---\n"
#                 report += f"Nivel de consenso: {st.session_state.consensus_level:.3f}\n"
#                 report += f"N√∫mero de argumentos evaluados: {len(st.session_state.arguments_for_evaluation)}\n"
# 
#             if hasattr(st.session_state, 'synthesis'):
#                 report += f"\n--- S√çNTESIS DEL MODERADOR ---\n"
#                 report += f"{st.session_state.synthesis}\n"
# 
#             report += f"\n{'='*70}\n--- FIN DEL REPORTE ---"
# 
#             st.download_button(
#                 label="‚¨áÔ∏è Descargar Reporte TXT",
#                 data=report,
#                 file_name=f"reporte_debate_{timestamp}.txt",
#                 mime="text/plain"
#             )
# 
#     with col2:
#         if st.session_state.gdn_completed:
#             # Exportar datos como JSON
#             export_data = {
#                 "config": {
#                     "topic": topic,
#                     "models": st.session_state.available_models,
#                     "timestamp": datetime.now().isoformat()
#                 },
#                 "responses": st.session_state.responses,
#                 "arguments": st.session_state.arguments_for_evaluation,
#                 "preferences": st.session_state.individual_preferences,
#                 "collective_preference": st.session_state.collective_preference,
#                 "consensus_level": st.session_state.consensus_level
#             }
# 
#             if hasattr(st.session_state, 'synthesis'):
#                 export_data["synthesis"] = st.session_state.synthesis
# 
#             st.download_button(
#                 label="‚¨áÔ∏è Descargar Datos JSON",
#                 data=json.dumps(export_data, indent=2, ensure_ascii=False),
#                 file_name=f"datos_debate_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
#                 mime="application/json"
#             )
# 
# # Footer
# st.markdown("---")
# st.markdown("### üîß Sistema de Debate Multi-Agente - TFM")
# st.markdown("Desarrollado con Streamlit, Hugging Face Transformers y Sentence Transformers")
#

!streamlit run app.py & npx localtunnel --port 8501